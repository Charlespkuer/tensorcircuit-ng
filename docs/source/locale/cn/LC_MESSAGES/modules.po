# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The TensorCircuit Authors
# This file is distributed under the same license as the tensorcircuit
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: tensorcircuit \n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-02-10 16:18+0800\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/modules.rst:2
msgid "tensorcircuit  API"
msgstr ""

#: ../../source/modules.rst:9
msgid "tensorcircuit.backends module"
msgstr ""

#: ../../source/modules.rst:17
msgid "tensorcircuit.backends.backend_factory module"
msgstr ""

#: of tensorcircuit.backends.backend_factory:1
msgid "backend register"
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend:1
msgid "Get the `tc.backend` object"
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend
#: tensorcircuit.backends.jax_backend.JaxBackend.argmax
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin
#: tensorcircuit.backends.jax_backend.JaxBackend.cast
#: tensorcircuit.backends.jax_backend.JaxBackend.concat
#: tensorcircuit.backends.jax_backend.JaxBackend.cond
#: tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix
#: tensorcircuit.backends.jax_backend.JaxBackend.copy
#: tensorcircuit.backends.jax_backend.JaxBackend.cos
#: tensorcircuit.backends.jax_backend.JaxBackend.cumsum
#: tensorcircuit.backends.jax_backend.JaxBackend.expm
#: tensorcircuit.backends.jax_backend.JaxBackend.grad
#: tensorcircuit.backends.jax_backend.JaxBackend.i
#: tensorcircuit.backends.jax_backend.JaxBackend.imag
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.is_sparse
#: tensorcircuit.backends.jax_backend.JaxBackend.is_tensor
#: tensorcircuit.backends.jax_backend.JaxBackend.jit
#: tensorcircuit.backends.jax_backend.JaxBackend.jvp
#: tensorcircuit.backends.jax_backend.JaxBackend.kron
#: tensorcircuit.backends.jax_backend.JaxBackend.max
#: tensorcircuit.backends.jax_backend.JaxBackend.min
#: tensorcircuit.backends.jax_backend.JaxBackend.numpy
#: tensorcircuit.backends.jax_backend.JaxBackend.onehot
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split
#: tensorcircuit.backends.jax_backend.JaxBackend.real
#: tensorcircuit.backends.jax_backend.JaxBackend.relu
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter
#: tensorcircuit.backends.jax_backend.JaxBackend.set_random_state
#: tensorcircuit.backends.jax_backend.JaxBackend.sin
#: tensorcircuit.backends.jax_backend.JaxBackend.size
#: tensorcircuit.backends.jax_backend.JaxBackend.softmax
#: tensorcircuit.backends.jax_backend.JaxBackend.solve
#: tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul
#: tensorcircuit.backends.jax_backend.JaxBackend.stack
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient
#: tensorcircuit.backends.jax_backend.JaxBackend.switch
#: tensorcircuit.backends.jax_backend.JaxBackend.tile
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dense
#: tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vjp
#: tensorcircuit.backends.jax_backend.JaxBackend.vmap
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast
#: tensorcircuit.backends.numpy_backend.NumpyBackend.concat
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter
#: tensorcircuit.backends.numpy_backend.NumpyBackend.set_random_state
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.concat
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.concat
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.set_random_state
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap
#: tensorcircuit.channels.amplitudedampingchannel
#: tensorcircuit.channels.depolarizingchannel
#: tensorcircuit.channels.kraus_to_super_gate
#: tensorcircuit.channels.phasedampingchannel
#: tensorcircuit.channels.single_qubit_kraus_identity_check
#: tensorcircuit.circuit.Circuit.__init__
#: tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply
#: tensorcircuit.circuit.Circuit.expectation
#: tensorcircuit.circuit.Circuit.general_kraus
#: tensorcircuit.circuit.Circuit.measure
#: tensorcircuit.circuit.Circuit.measure_jit
#: tensorcircuit.circuit.Circuit.mid_measurement
#: tensorcircuit.circuit.Circuit.replace_inputs
#: tensorcircuit.circuit.Circuit.replace_mps_inputs
#: tensorcircuit.circuit.Circuit.wavefunction tensorcircuit.circuit.expectation
#: tensorcircuit.cons.get_contractor tensorcircuit.cons.get_dtype
#: tensorcircuit.cons.plain_contractor tensorcircuit.cons.set_contractor
#: tensorcircuit.cons.set_dtype tensorcircuit.cons.set_tensornetwork_backend
#: tensorcircuit.gates.any_gate tensorcircuit.gates.bmatrix
#: tensorcircuit.gates.cr_gate tensorcircuit.gates.exponential_gate
#: tensorcircuit.gates.exponential_gate_unity tensorcircuit.gates.iswap_gate
#: tensorcircuit.gates.matrix_for_gate tensorcircuit.gates.num_to_tensor
#: tensorcircuit.gates.r_gate tensorcircuit.gates.rgate_theoretical
#: tensorcircuit.gates.rx_gate tensorcircuit.gates.ry_gate
#: tensorcircuit.gates.rz_gate tensorcircuit.keras.QuantumLayer.__init__
#: tensorcircuit.keras.load_func tensorcircuit.keras.output_asis_loss
#: tensorcircuit.keras.save_func tensorcircuit.mpscircuit.MPSCircuit.__init__
#: tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate
#: tensorcircuit.mpscircuit.MPSCircuit.apply_double_gate
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply
#: tensorcircuit.mpscircuit.MPSCircuit.apply_single_gate
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_double_gates
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product
#: tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction
#: tensorcircuit.mpscircuit.MPSCircuit.general_expectation
#: tensorcircuit.mpscircuit.MPSCircuit.measure
#: tensorcircuit.mpscircuit.MPSCircuit.mid_measurement
#: tensorcircuit.mpscircuit.MPSCircuit.position
#: tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps
#: tensorcircuit.mpscircuit.MPSCircuit.set_truncation_rule
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction
#: tensorcircuit.mpscircuit.split_tensor
#: tensorcircuit.quantum.QuAdjointVector.__init__
#: tensorcircuit.quantum.QuAdjointVector.from_tensor
#: tensorcircuit.quantum.QuOperator.__init__
#: tensorcircuit.quantum.QuOperator.contract
#: tensorcircuit.quantum.QuOperator.eval
#: tensorcircuit.quantum.QuOperator.from_tensor
#: tensorcircuit.quantum.QuOperator.partial_trace
#: tensorcircuit.quantum.QuOperator.tensor_product
#: tensorcircuit.quantum.QuScalar.__init__
#: tensorcircuit.quantum.QuScalar.from_tensor
#: tensorcircuit.quantum.QuVector.__init__
#: tensorcircuit.quantum.QuVector.from_tensor
#: tensorcircuit.quantum.eliminate_identities tensorcircuit.quantum.entropy
#: tensorcircuit.quantum.generate_local_hamiltonian
#: tensorcircuit.quantum.identity tensorcircuit.quantum.measurement_counts
#: tensorcircuit.quantum.quantum_constructor
#: tensorcircuit.quantum.reduced_density_matrix
#: tensorcircuit.simplify.infer_new_shape
#: tensorcircuit.simplify.pseudo_contract_between
#: tensorcircuit.templates.graphs.Line1D
#: tensorcircuit.templates.measurements.any_measurements
#: tensorcircuit.templates.measurements.sparse_expectation
#: tensorcircuit.utils.return_partial tensorcircuit.vis.render_pdf
msgid "Parameters"
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend:3
msgid "\"numpy\", \"tensorflow\", \"jax\", \"pytorch\""
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend
#: tensorcircuit.circuit.Circuit.expectation tensorcircuit.circuit.expectation
#: tensorcircuit.cons.get_contractor tensorcircuit.cons.set_contractor
#: tensorcircuit.gates.bmatrix tensorcircuit.keras.load_func
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate
#: tensorcircuit.quantum.QuOperator.__init__
#: tensorcircuit.quantum.QuOperator.eval tensorcircuit.quantum.check_spaces
msgid "Raises"
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend:5
msgid "Backend doesn't exist for `backend` argument."
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend
#: tensorcircuit.backends.jax_backend.JaxBackend.argmax
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin
#: tensorcircuit.backends.jax_backend.JaxBackend.cast
#: tensorcircuit.backends.jax_backend.JaxBackend.cond
#: tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix
#: tensorcircuit.backends.jax_backend.JaxBackend.copy
#: tensorcircuit.backends.jax_backend.JaxBackend.cos
#: tensorcircuit.backends.jax_backend.JaxBackend.cumsum
#: tensorcircuit.backends.jax_backend.JaxBackend.expm
#: tensorcircuit.backends.jax_backend.JaxBackend.grad
#: tensorcircuit.backends.jax_backend.JaxBackend.i
#: tensorcircuit.backends.jax_backend.JaxBackend.imag
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.is_sparse
#: tensorcircuit.backends.jax_backend.JaxBackend.is_tensor
#: tensorcircuit.backends.jax_backend.JaxBackend.jit
#: tensorcircuit.backends.jax_backend.JaxBackend.jvp
#: tensorcircuit.backends.jax_backend.JaxBackend.kron
#: tensorcircuit.backends.jax_backend.JaxBackend.max
#: tensorcircuit.backends.jax_backend.JaxBackend.min
#: tensorcircuit.backends.jax_backend.JaxBackend.numpy
#: tensorcircuit.backends.jax_backend.JaxBackend.onehot
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split
#: tensorcircuit.backends.jax_backend.JaxBackend.real
#: tensorcircuit.backends.jax_backend.JaxBackend.relu
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter
#: tensorcircuit.backends.jax_backend.JaxBackend.sin
#: tensorcircuit.backends.jax_backend.JaxBackend.size
#: tensorcircuit.backends.jax_backend.JaxBackend.softmax
#: tensorcircuit.backends.jax_backend.JaxBackend.solve
#: tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul
#: tensorcircuit.backends.jax_backend.JaxBackend.stack
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient
#: tensorcircuit.backends.jax_backend.JaxBackend.switch
#: tensorcircuit.backends.jax_backend.JaxBackend.tile
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dense
#: tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vjp
#: tensorcircuit.backends.jax_backend.JaxBackend.vmap
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap
#: tensorcircuit.channels.amplitudedampingchannel
#: tensorcircuit.channels.depolarizingchannel
#: tensorcircuit.channels.kraus_to_super_gate
#: tensorcircuit.channels.phasedampingchannel
#: tensorcircuit.channels.resetchannel
#: tensorcircuit.circuit.Circuit.expectation
#: tensorcircuit.circuit.Circuit.is_valid tensorcircuit.circuit.Circuit.measure
#: tensorcircuit.circuit.Circuit.measure_jit
#: tensorcircuit.circuit.Circuit.perfect_sampling
#: tensorcircuit.circuit.Circuit.wavefunction tensorcircuit.circuit.expectation
#: tensorcircuit.cons.get_contractor tensorcircuit.cons.plain_contractor
#: tensorcircuit.cons.set_contractor
#: tensorcircuit.cons.set_tensornetwork_backend tensorcircuit.gates.any_gate
#: tensorcircuit.gates.bmatrix tensorcircuit.gates.cr_gate
#: tensorcircuit.gates.exponential_gate
#: tensorcircuit.gates.exponential_gate_unity tensorcircuit.gates.iswap_gate
#: tensorcircuit.gates.matrix_for_gate tensorcircuit.gates.num_to_tensor
#: tensorcircuit.gates.r_gate tensorcircuit.gates.random_single_qubit_gate
#: tensorcircuit.gates.random_two_qubit_gate
#: tensorcircuit.gates.rgate_theoretical tensorcircuit.gates.rx_gate
#: tensorcircuit.gates.ry_gate tensorcircuit.gates.rz_gate
#: tensorcircuit.keras.load_func tensorcircuit.keras.output_asis_loss
#: tensorcircuit.mpscircuit.MPSCircuit.conj
#: tensorcircuit.mpscircuit.MPSCircuit.copy
#: tensorcircuit.mpscircuit.MPSCircuit.copy_without_tensor
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product
#: tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction
#: tensorcircuit.mpscircuit.MPSCircuit.general_expectation
#: tensorcircuit.mpscircuit.MPSCircuit.get_norm
#: tensorcircuit.mpscircuit.MPSCircuit.is_valid
#: tensorcircuit.mpscircuit.MPSCircuit.measure
#: tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction
#: tensorcircuit.mpscircuit.split_tensor
#: tensorcircuit.quantum.QuAdjointVector.from_tensor
#: tensorcircuit.quantum.QuOperator.contract
#: tensorcircuit.quantum.QuOperator.eval
#: tensorcircuit.quantum.QuOperator.from_tensor
#: tensorcircuit.quantum.QuOperator.partial_trace
#: tensorcircuit.quantum.QuOperator.tensor_product
#: tensorcircuit.quantum.QuScalar.from_tensor
#: tensorcircuit.quantum.QuVector.from_tensor
#: tensorcircuit.quantum.eliminate_identities tensorcircuit.quantum.entropy
#: tensorcircuit.quantum.generate_local_hamiltonian
#: tensorcircuit.quantum.identity tensorcircuit.quantum.measurement_counts
#: tensorcircuit.quantum.quantum_constructor
#: tensorcircuit.quantum.reduced_density_matrix
#: tensorcircuit.quantum.trace_product tensorcircuit.simplify.infer_new_shape
#: tensorcircuit.simplify.pseudo_contract_between
#: tensorcircuit.templates.graphs.Line1D
#: tensorcircuit.templates.measurements.any_measurements
#: tensorcircuit.templates.measurements.sparse_expectation
#: tensorcircuit.utils.return_partial tensorcircuit.vis.render_pdf
msgid "Returns"
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend:6
#: tensorcircuit.cons.set_tensornetwork_backend:32
msgid "The `tc.backend` object that with all registered universal functions."
msgstr ""

#: of tensorcircuit.backends.backend_factory.get_backend
#: tensorcircuit.backends.jax_backend.JaxBackend.argmax
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin
#: tensorcircuit.backends.jax_backend.JaxBackend.cast
#: tensorcircuit.backends.jax_backend.JaxBackend.cond
#: tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix
#: tensorcircuit.backends.jax_backend.JaxBackend.copy
#: tensorcircuit.backends.jax_backend.JaxBackend.cos
#: tensorcircuit.backends.jax_backend.JaxBackend.cumsum
#: tensorcircuit.backends.jax_backend.JaxBackend.expm
#: tensorcircuit.backends.jax_backend.JaxBackend.grad
#: tensorcircuit.backends.jax_backend.JaxBackend.i
#: tensorcircuit.backends.jax_backend.JaxBackend.imag
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.is_sparse
#: tensorcircuit.backends.jax_backend.JaxBackend.is_tensor
#: tensorcircuit.backends.jax_backend.JaxBackend.jit
#: tensorcircuit.backends.jax_backend.JaxBackend.jvp
#: tensorcircuit.backends.jax_backend.JaxBackend.kron
#: tensorcircuit.backends.jax_backend.JaxBackend.max
#: tensorcircuit.backends.jax_backend.JaxBackend.min
#: tensorcircuit.backends.jax_backend.JaxBackend.numpy
#: tensorcircuit.backends.jax_backend.JaxBackend.onehot
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split
#: tensorcircuit.backends.jax_backend.JaxBackend.real
#: tensorcircuit.backends.jax_backend.JaxBackend.relu
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter
#: tensorcircuit.backends.jax_backend.JaxBackend.sin
#: tensorcircuit.backends.jax_backend.JaxBackend.size
#: tensorcircuit.backends.jax_backend.JaxBackend.softmax
#: tensorcircuit.backends.jax_backend.JaxBackend.solve
#: tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul
#: tensorcircuit.backends.jax_backend.JaxBackend.stack
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu
#: tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient
#: tensorcircuit.backends.jax_backend.JaxBackend.switch
#: tensorcircuit.backends.jax_backend.JaxBackend.tile
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dense
#: tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad
#: tensorcircuit.backends.jax_backend.JaxBackend.vjp
#: tensorcircuit.backends.jax_backend.JaxBackend.vmap
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap
#: tensorcircuit.channels.amplitudedampingchannel
#: tensorcircuit.channels.depolarizingchannel
#: tensorcircuit.channels.kraus_to_super_gate
#: tensorcircuit.channels.phasedampingchannel
#: tensorcircuit.channels.resetchannel
#: tensorcircuit.circuit.Circuit.expectation
#: tensorcircuit.circuit.Circuit.is_valid tensorcircuit.circuit.Circuit.measure
#: tensorcircuit.circuit.Circuit.measure_jit
#: tensorcircuit.circuit.Circuit.perfect_sampling
#: tensorcircuit.circuit.Circuit.wavefunction tensorcircuit.circuit.expectation
#: tensorcircuit.cons.get_contractor tensorcircuit.cons.plain_contractor
#: tensorcircuit.cons.set_contractor
#: tensorcircuit.cons.set_tensornetwork_backend tensorcircuit.gates.any_gate
#: tensorcircuit.gates.bmatrix tensorcircuit.gates.cr_gate
#: tensorcircuit.gates.exponential_gate
#: tensorcircuit.gates.exponential_gate_unity tensorcircuit.gates.iswap_gate
#: tensorcircuit.gates.matrix_for_gate tensorcircuit.gates.num_to_tensor
#: tensorcircuit.gates.r_gate tensorcircuit.gates.random_single_qubit_gate
#: tensorcircuit.gates.random_two_qubit_gate
#: tensorcircuit.gates.rgate_theoretical tensorcircuit.gates.rx_gate
#: tensorcircuit.gates.ry_gate tensorcircuit.gates.rz_gate
#: tensorcircuit.keras.load_func tensorcircuit.keras.output_asis_loss
#: tensorcircuit.mpscircuit.MPSCircuit.conj
#: tensorcircuit.mpscircuit.MPSCircuit.copy
#: tensorcircuit.mpscircuit.MPSCircuit.copy_without_tensor
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product
#: tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction
#: tensorcircuit.mpscircuit.MPSCircuit.general_expectation
#: tensorcircuit.mpscircuit.MPSCircuit.get_norm
#: tensorcircuit.mpscircuit.MPSCircuit.is_valid
#: tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction
#: tensorcircuit.mpscircuit.split_tensor
#: tensorcircuit.quantum.QuAdjointVector.from_tensor
#: tensorcircuit.quantum.QuOperator.contract
#: tensorcircuit.quantum.QuOperator.eval
#: tensorcircuit.quantum.QuOperator.from_tensor
#: tensorcircuit.quantum.QuOperator.partial_trace
#: tensorcircuit.quantum.QuOperator.tensor_product
#: tensorcircuit.quantum.QuScalar.from_tensor
#: tensorcircuit.quantum.QuVector.from_tensor
#: tensorcircuit.quantum.eliminate_identities tensorcircuit.quantum.entropy
#: tensorcircuit.quantum.generate_local_hamiltonian
#: tensorcircuit.quantum.identity tensorcircuit.quantum.measurement_counts
#: tensorcircuit.quantum.quantum_constructor
#: tensorcircuit.quantum.reduced_density_matrix
#: tensorcircuit.quantum.trace_product tensorcircuit.simplify.infer_new_shape
#: tensorcircuit.simplify.pseudo_contract_between
#: tensorcircuit.templates.graphs.Line1D
#: tensorcircuit.templates.measurements.any_measurements
#: tensorcircuit.templates.measurements.sparse_expectation
#: tensorcircuit.utils.return_partial tensorcircuit.vis.render_pdf
msgid "Return type"
msgstr ""

#: ../../source/modules.rst:25
msgid "tensorcircuit.backends.jax_backend module"
msgstr ""

#: of tensorcircuit.backends.jax_backend:1
msgid "backend magic inherited from tensornetwork: jax backend"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend:1
msgid "Bases: :class:`tensornetwork.backends.jax.jax_backend.JaxBackend`"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend:1
msgid ""
"see the original backend API at `jax backend "
"<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/jax/jax_backend.py>`_"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.abs:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.abs:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.abs:1
msgid "Returns the elementwise absolute value of tensor. Args:"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.abs:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.abs:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.abs:3
msgid "tensor: An input tensor."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.abs:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.abs:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.abs:4
msgid "Returns:"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.abs:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.abs:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.abs:5
msgid "tensor: Its elementwise absolute value."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.argmax:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax:1
msgid "Return the index of maximum of an array an axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.argmax:3
#: tensorcircuit.backends.jax_backend.JaxBackend.argmax:7
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin:3
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin:7
#: tensorcircuit.backends.jax_backend.JaxBackend.concat:3
#: tensorcircuit.backends.jax_backend.JaxBackend.cond:3
#: tensorcircuit.backends.jax_backend.JaxBackend.cond:5
#: tensorcircuit.backends.jax_backend.JaxBackend.cond:7
#: tensorcircuit.backends.jax_backend.JaxBackend.cond:9
#: tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix:10
#: tensorcircuit.backends.jax_backend.JaxBackend.cumsum:3
#: tensorcircuit.backends.jax_backend.JaxBackend.cumsum:8
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:3
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:11
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:11
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:11
#: tensorcircuit.backends.jax_backend.JaxBackend.is_sparse:3
#: tensorcircuit.backends.jax_backend.JaxBackend.is_sparse:5
#: tensorcircuit.backends.jax_backend.JaxBackend.max:3
#: tensorcircuit.backends.jax_backend.JaxBackend.max:7
#: tensorcircuit.backends.jax_backend.JaxBackend.min:3
#: tensorcircuit.backends.jax_backend.JaxBackend.min:7
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split:4
#: tensorcircuit.backends.jax_backend.JaxBackend.random_split:6
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter:3
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter:5
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter:7
#: tensorcircuit.backends.jax_backend.JaxBackend.scatter:9
#: tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul:3
#: tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul:5
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:3
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:11
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:3
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:15
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:13
#: tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient:3
#: tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient:5
#: tensorcircuit.backends.jax_backend.JaxBackend.switch:3
#: tensorcircuit.backends.jax_backend.JaxBackend.switch:5
#: tensorcircuit.backends.jax_backend.JaxBackend.switch:7
#: tensorcircuit.backends.jax_backend.JaxBackend.tile:3
#: tensorcircuit.backends.jax_backend.JaxBackend.tile:7
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dense:3
#: tensorcircuit.backends.jax_backend.JaxBackend.to_dense:5
#: tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts:3
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:29
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:36
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.concat:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix:10
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:15
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:29
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:36
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.concat:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:29
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:36
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.concat:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix:10
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:15
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:29
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:36
#: tensorcircuit.circuit.Circuit.expectation:16
#: tensorcircuit.circuit.expectation:42 tensorcircuit.circuit.expectation:50
#: tensorcircuit.circuit.expectation:51
#: tensorcircuit.keras.QuantumLayer.__init__:4
#: tensorcircuit.keras.QuantumLayer.__init__:6
#: tensorcircuit.keras.output_asis_loss:3
#: tensorcircuit.keras.output_asis_loss:5
#: tensorcircuit.keras.output_asis_loss:7 tensorcircuit.quantum.entropy:3
#: tensorcircuit.quantum.entropy:7
#: tensorcircuit.quantum.generate_local_hamiltonian:4
#: tensorcircuit.quantum.generate_local_hamiltonian:8
#: tensorcircuit.quantum.reduced_density_matrix:3
#: tensorcircuit.quantum.reduced_density_matrix:5
#: tensorcircuit.quantum.reduced_density_matrix:9
#: tensorcircuit.simplify.pseudo_contract_between:3
#: tensorcircuit.simplify.pseudo_contract_between:5
#: tensorcircuit.simplify.pseudo_contract_between:7
#: tensorcircuit.templates.graphs.Line1D:3
#: tensorcircuit.templates.graphs.Line1D:7
#: tensorcircuit.templates.measurements.any_measurements:4
#: tensorcircuit.templates.measurements.any_measurements:12
#: tensorcircuit.templates.measurements.sparse_expectation:3
msgid "[description]"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.argmax:5
#: tensorcircuit.backends.jax_backend.JaxBackend.argmin:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmax:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmax:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmax:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin:5
msgid "[description], defaults to 0, different behavior from numpy defaults!"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.argmin:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.argmin:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.argmin:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.argmin:1
msgid "Return the index of minimum of an array an axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cast:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast:1
msgid "Cast the tensor dtype of a ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cast:3
#: tensorcircuit.backends.jax_backend.JaxBackend.imag:3
#: tensorcircuit.backends.jax_backend.JaxBackend.real:3
#: tensorcircuit.backends.jax_backend.JaxBackend.size:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size:3
msgid "tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cast:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast:5
msgid "\"float32\", \"float64\", \"complex64\", \"complex128\""
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cast:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cast:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cast:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cast:7
msgid "``a`` of new dtype"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.concat:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.concat:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.concat:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.concat:1
msgid "Join a sequence of arrays along an existing axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.concat:5
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:5
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:5
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:9
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:7
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:31
#: tensorcircuit.backends.numpy_backend.NumpyBackend.concat:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:31
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.concat:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:31
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.concat:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:31
msgid "[description], defaults to 0"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cond:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cond:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cond:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cond:1
msgid ""
"the native cond for XLA compiling, wrapper for ``tf.cond`` and limited "
"functionality of ``jax.lax.cond``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.convert_to_tensor:1
msgid "Convert a np.array or a tensor to a tensor type for the backend."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix:1
msgid ""
"generate coo format sparse matrix from indices and values, the only "
"sparse format supported in different ML backends"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix:4
msgid "shape [n, 2] for n non zero values in the returned matrix"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix:6
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix:6
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix:6
msgid "shape [n]"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.coo_sparse_matrix:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.coo_sparse_matrix:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.coo_sparse_matrix:8
msgid "Tuple[int, ...]"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.copy:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy:1
msgid "Return the expm of ``a``, matrix exponential."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.copy:3
#: tensorcircuit.backends.jax_backend.JaxBackend.cos:3
#: tensorcircuit.backends.jax_backend.JaxBackend.expm:3
#: tensorcircuit.backends.jax_backend.JaxBackend.kron:3
#: tensorcircuit.backends.jax_backend.JaxBackend.kron:5
#: tensorcircuit.backends.jax_backend.JaxBackend.numpy:3
#: tensorcircuit.backends.jax_backend.JaxBackend.sin:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin:3
msgid "tensor in matrix form"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.copy:5
#: tensorcircuit.backends.jax_backend.JaxBackend.expm:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.copy:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.copy:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.copy:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm:5
msgid "matrix exponential of matrix ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cos:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos:1
msgid "Return the cosine of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cos:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cos:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cos:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cos:5
msgid "cosine of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cumsum:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum:1
msgid "Return the cumulative sum of the elements along a given axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.cumsum:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.cumsum:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.cumsum:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.cumsum:5
msgid ""
"The default behavior is the same as numpy, different from tf/torch as "
"cumsum of the flattern 1D array, defaults to None"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.expm:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.expm:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.expm:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.expm:1
msgid "Return the copy of tensor ''a''."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.eye:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eye:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eye:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eye:4
msgid "Return an identity matrix of dimension `dim`"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.eye:2
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eye:2
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eye:2
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eye:2
msgid ""
"Depending on specific backends, `dim` has to be either an int (numpy, "
"torch, tensorflow) or a `ShapeType` object (for block-sparse backends). "
"Block-sparse behavior is currently not supported"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.eye:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eye:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eye:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eye:8
#: tensorcircuit.keras.QuantumLayer.build:11
msgid "Args:"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.eye:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.eye:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.eye:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.eye:7
msgid ""
"N (int): The dimension of the returned matrix. dtype: The dtype of the "
"returned matrix. M (int): The dimension of the returned matrix."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.grad:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad:1
msgid "Return function which is the grad function of input ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.grad:12
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad:12
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad:12
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad:12
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad:12
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad:12
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad:12
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad:12
msgid "function to be differentiated"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.grad:14
#: tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad:14
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad:14
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad:14
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad:14
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad:14
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad:14
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad:14
msgid "the position of args in ``f`` that are to be differentiated, defaults to 0"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.grad:16
#: tensorcircuit.backends.numpy_backend.NumpyBackend.grad:16
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.grad:16
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.grad:16
msgid "the grad fuction of ``f`` with the same set of arguments as ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.i:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i:1
msgid "Return 1.j in as a tensor compatible with the backend."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.i:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i:3
msgid "\"complex64\" or \"complex128\""
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.i:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.i:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.i:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.i:5
msgid "1.j tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.imag:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag:1
msgid "Return the elementwise imaginary value of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.imag:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.imag:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.imag:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.imag:5
msgid "imaginary value of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:1
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:1
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:1
#: tensorcircuit.templates.measurements.sparse_expectation:1
msgid "[summary]"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:5
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:5
msgid "The possible options"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:7
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:7
msgid "Sampling output shape"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randc:9
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randc:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randc:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randc:9
msgid ""
"probability for each option in a, defaults to None, as equal probability "
"distribution"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:1
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:1
msgid ""
"Call random normal function with the random state management behind the "
"scene."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:3
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:7
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:3
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:7
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:11
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:9
msgid "[description], defaults to 1"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.implicit_randn:9
#: tensorcircuit.backends.jax_backend.JaxBackend.implicit_randu:9
msgid "[description], defaults to \"32\""
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.is_sparse:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_sparse:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_sparse:1
msgid "determine whether the type of input ``a`` is of sparse type"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.is_tensor:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor:1
msgid "Return a boolean on whether ``a`` is a tensor in backend package."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.is_tensor:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor:3
msgid "a tensor to be determined"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.is_tensor:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.is_tensor:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.is_tensor:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.is_tensor:5
msgid "whether ``a`` is a tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jit:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit:1
msgid "Return jitted version function of ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jit:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit:3
msgid "function to be jitted"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jit:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit:5
msgid "index of args that doesn't regarded as tensor, only work for jax backend"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jit:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit:8
msgid ""
"whether open XLA compliation, only works for tensorflow backend, defaults"
" False since several ops has no XLA correspondence"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jit:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jit:11
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jit:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jit:11
msgid "jitted ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jvp:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp:1
msgid ""
"Function that computes a (forward-mode) Jacobian-vector product of ``f``."
" Strictly speaking, this function is value_and_jvp"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jvp:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp:4
msgid "The function to compute jvp"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jvp:6
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp:6
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp:6
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp:6
msgid "primals"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jvp:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp:8
msgid "tangents"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.jvp:10
#: tensorcircuit.backends.numpy_backend.NumpyBackend.jvp:10
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.jvp:10
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.jvp:10
msgid ""
"(``f(*inputs)``, jvp_tensor), where jvp_tensor is the same shape as "
"output of ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.kron:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron:1
msgid "Return the kronecker product of two matrices ``a`` and ``b``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.kron:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.kron:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.kron:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.kron:7
msgid "kronecker product of ``a`` and ``b``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.max:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max:1
msgid "Return the maximum of an array or maximum along an axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.max:5
#: tensorcircuit.backends.jax_backend.JaxBackend.min:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.max:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.max:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.max:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min:5
#: tensorcircuit.keras.QuantumLayer.__init__:10
#: tensorcircuit.quantum.reduced_density_matrix:7
msgid "[description], defaults to None"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.min:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.min:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.min:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.min:1
msgid "Return the minimum of an array or minimum along an axis."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.numpy:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy:1
msgid ""
"Return the numpy array of a tensor ``a``, but may not work in a jitted "
"function."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.numpy:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.numpy:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.numpy:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.numpy:5
msgid "numpy array of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.onehot:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot:1
msgid ""
"One-hot encodes the given ``a``. Each index in the input ``a`` is encoded"
" as a vector of zeros of length ``num`` with the element at index set to "
"one:"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.onehot:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot:5
msgid "input tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.onehot:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot:7
msgid "number of features in onehot dimension"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.onehot:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.onehot:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.onehot:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.onehot:9
msgid "onehot tensor with the last extra dimension"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.ones:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.ones:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.ones:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.ones:1
msgid ""
"Return an ones-matrix of dimension `dim` Depending on specific backends, "
"`dim` has to be either an int (numpy, torch, tensorflow) or a `ShapeType`"
" object (for block-sparse backends). Block-sparse behavior is currently "
"not supported Args:"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.ones:7
#: tensorcircuit.backends.jax_backend.JaxBackend.zeros:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.ones:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.zeros:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.ones:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.zeros:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.ones:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.zeros:8
msgid ""
"shape (int): The dimension of the returned matrix. dtype: The dtype of "
"the returned matrix."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.random_split:1
msgid ""
"A jax like split API, but does't split the key generator for other "
"backends. just for a consistent interface of random code, be careful that"
" you know what the function actually does."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.real:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real:1
msgid "Return the elementwise real value of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.real:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.real:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.real:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.real:5
msgid "real value of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.relu:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu:1
msgid ""
"Rectified linear unit activation function. Computes the element-wise "
"function:"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.relu:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu:4
msgid "\\mathrm{relu}(x)=\\max(x,0)"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.relu:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu:9
msgid "Input tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.relu:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.relu:11
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.relu:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.relu:11
msgid "Tensor after relu"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.scatter:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.scatter:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.scatter:1
msgid ""
"Roughly equivalent to operand[indices] = updates, indices only support "
"shape with rank 2 for now"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.set_random_state:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.set_random_state:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.set_random_state:1
msgid "Set the random state attached in the backend"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.set_random_state:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.set_random_state:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.set_random_state:3
msgid "int, defaults to None"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sin:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin:1
msgid "Return the  elementwise sine of a tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sin:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sin:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.sin:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sin:5
msgid "sine of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.size:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size:1
msgid "Return the total number of elements in ``a`` in tensor form."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.size:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.size:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.size:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.size:5
msgid "the total number of elements in ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.softmax:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax:1
msgid ""
"Softmax function. Computes the function which rescales elements to the "
"range [0,1] such that the elements along axis sum to 1."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.softmax:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax:4
msgid "\\mathrm{softmax}(x) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.softmax:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax:9
msgid "Tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.softmax:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax:11
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax:11
msgid ""
"A dimension along which Softmax will be computed , defaults to None for "
"all axis sum."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.softmax:13
#: tensorcircuit.backends.jax_backend.JaxBackend.stack:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.softmax:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.softmax:13
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.softmax:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack:7
msgid "concatenated tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.solve:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve:1
msgid "Solve the linear system Ax=b and return x"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.solve:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve:3
msgid "The multiplied matrix."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.solve:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve:5
msgid "The resulted matrix."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.solve:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.solve:7
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.solve:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.solve:7
msgid "The solution of the linear system."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul:1
msgid "sparse matrix times dense matrix"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.sparse_dense_matmul:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.sparse_dense_matmul:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.sparse_dense_matmul:7
msgid "dense matrix"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stack:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack:1
msgid "Concatenates a sequence of tensors ``a`` along a new dimension ``axis``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stack:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack:3
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack:3
msgid "List of tensors in the same shape"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stack:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stack:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stack:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stack:5
msgid "the stack axis, defaults to 0"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:5
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:3
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:3
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:3
msgid "stateful register for each package"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:7
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:7
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:7
msgid "shape of output sampling tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stateful_randn:13
#: tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:11
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randn:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:11
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randn:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:11
msgid "only real data type is supported, \"32\" or \"64\", defaults to \"32\""
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:1
msgid "Uniform random sampler from ``low`` to ``high``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stateful_randu:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stateful_randu:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stateful_randu:5
msgid "shape of output sampling tensor, defaults to 1"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.stop_gradient:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.stop_gradient:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.stop_gradient:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.stop_gradient:1
msgid "stop backpropagation from a"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.switch:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.switch:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.switch:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.switch:1
msgid "``branches[index]()``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.tile:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile:1
msgid "Constructs a tensor by tiling a given tensor."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.tile:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.tile:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.tile:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.tile:5
msgid "1d tensor with length the same as the rank of ``a``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.to_dense:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.to_dense:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.to_dense:1
msgid "convert sparse matrix to dense tensor"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts:1
msgid ""
"Find the unique elements and their corresponding counts of the given "
"tensor ``a``."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.unique_with_counts:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.unique_with_counts:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.unique_with_counts:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.unique_with_counts:5
msgid "Unique elements, corresponding counts"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad:1
msgid "Return function which returns the value and grad of ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.value_and_grad:16
#: tensorcircuit.backends.numpy_backend.NumpyBackend.value_and_grad:16
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.value_and_grad:16
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.value_and_grad:16
msgid ""
"the value and grad fuction of ``f`` with the same set of arguments as "
"``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:1
msgid ""
"Return vvag function of ``f``. the inputs for ``f`` is (args[0], args[1],"
" args[2], ...), and the output of ``f`` is a scalar. Suppose vvag(f) is a"
" function with inputs in the form (vargs[0], args[1], args[2], ...), "
"where vagrs[0] has one extra dimension than args[0] in the first axis and"
" consistent with args[0] in shape for remaining dimensions, i.e. "
"shape(vargs[0]) = [batch] + shape(args[0]). (We only cover case where "
"``vectorized_argnums`` defaults to 0 here for demonstration). vvag(f) "
"returns a tuple as a value tensor with shape [batch, 1] and a gradient "
"tuple with shape: ([batch]+shape(args[argnum]) for argnum in argnums). "
"The gradient for argnums=k is defined as"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:9
msgid ""
"g^k = \\frac{\\partial \\sum_{i\\in batch} f(vargs[0][i], args[1], "
"...)}{\\partial args[k]}"
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:13
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:13
msgid "Therefore, if argnums=0, the gradient is reduced to"
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:15
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:15
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:15
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:15
msgid "g^0_i = \\frac{\\partial f(vargs[0][i])}{\\partial vargs[0][i]}"
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:19
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:19
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:19
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:19
msgid ""
", which is specifically suitable for batched VQE optimization, where "
"args[0] is the circuit parameters."
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:21
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:21
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:21
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:21
msgid "And if argnums=1, the gradient is like"
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:23
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:23
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:23
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:23
msgid ""
"g^1_i = \\frac{\\partial \\sum_j f(vargs[0][j], args[1])}{\\partial "
"args[1][i]}\n"
"\n"
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:26
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:26
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:26
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:26
msgid ""
", which is suitable for quantum machine learning scenarios, where ``f`` "
"is the loss function, args[0] corresponds the input data and args[1] "
"corresponds to the weights in the QML model."
msgstr ""

#: of
#: tensorcircuit.backends.jax_backend.JaxBackend.vectorized_value_and_grad:33
#: tensorcircuit.backends.jax_backend.JaxBackend.vmap:6
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vectorized_value_and_grad:33
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap:6
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vectorized_value_and_grad:33
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap:6
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vectorized_value_and_grad:33
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap:6
msgid ""
"the args to be vectorized, these arguments should share the same batch "
"shape in the fist dimension"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vjp:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp:1
msgid ""
"Function that computes the dot product between a vector v and the "
"Jacobian of the given function at the point given by the inputs. (reverse"
" mode AD relevant) Strictly speaking, this function is value_and_vjp."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vjp:6
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp:6
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp:6
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp:6
msgid "The function to carry out vjp calculation"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vjp:8
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp:8
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp:8
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp:8
msgid "input for ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vjp:10
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp:10
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp:10
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp:10
msgid ""
"value vector or gradient from downstream in reserse mode AD the same "
"shape as return of function ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vjp:13
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vjp:13
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vjp:13
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vjp:13
msgid "(``f(*inputs)``, vjp_tensor), where vjp_tensor is the same shape as inputs"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vmap:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap:1
msgid ""
"Return vectorized map or batched version of ``f`` on the first extra "
"axis, the general interface support ``f`` with multiple arguments and "
"broadcast in the fist dimension."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vmap:4
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap:4
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap:4
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap:4
msgid "function to be broadcasted."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.vmap:9
#: tensorcircuit.backends.numpy_backend.NumpyBackend.vmap:9
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.vmap:9
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.vmap:9
msgid "vmap version of ``f``"
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.zeros:1
#: tensorcircuit.backends.numpy_backend.NumpyBackend.zeros:1
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.zeros:1
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.zeros:1
msgid ""
"Return a zeros-matrix of dimension `dim` Depending on specific backends, "
"`dim` has to be either an int (numpy, torch, tensorflow) or a `ShapeType`"
" object (for block-sparse backends)."
msgstr ""

#: of tensorcircuit.backends.jax_backend.JaxBackend.zeros:5
#: tensorcircuit.backends.numpy_backend.NumpyBackend.zeros:5
#: tensorcircuit.backends.pytorch_backend.PyTorchBackend.zeros:5
#: tensorcircuit.backends.tensorflow_backend.TensorFlowBackend.zeros:5
msgid "Block-sparse behavior is currently not supported Args:"
msgstr ""

#: of tensorcircuit.backends.jax_backend.optax_optimizer:1
#: tensorcircuit.backends.tensorflow_backend.keras_optimizer:1
#: tensorcircuit.circuit.Circuit:1 tensorcircuit.densitymatrix.DMCircuit:1
#: tensorcircuit.gates.GateF:1 tensorcircuit.mpscircuit.MPSCircuit:1
#: tensorcircuit.quantum.QuOperator:1
#: tensorcircuit.templates.graphs.Grid2DCoord:1
msgid "Bases: :class:`object`"
msgstr ""

#: ../../source/modules.rst:33
msgid "tensorcircuit.backends.numpy_backend module"
msgstr ""

#: of tensorcircuit.backends.numpy_backend:1
msgid "backend magic inherited from tensornetwork: numpy backend"
msgstr ""

#: of tensorcircuit.backends.numpy_backend.NumpyBackend:1
msgid "Bases: :class:`tensornetwork.backends.numpy.numpy_backend.NumPyBackend`"
msgstr ""

#: of tensorcircuit.backends.numpy_backend.NumpyBackend:1
msgid ""
"see the original backend API at `numpy backend "
"<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/numpy/numpy_backend.py>`_"
msgstr ""

#: ../../source/modules.rst:41
msgid "tensorcircuit.backends.pytorch_backend module"
msgstr ""

#: of tensorcircuit.backends.pytorch_backend:1
msgid "backend magic inherited from tensornetwork: pytorch backend"
msgstr ""

#: of tensorcircuit.backends.pytorch_backend.PyTorchBackend:1
msgid ""
"Bases: "
":class:`tensornetwork.backends.pytorch.pytorch_backend.PyTorchBackend`"
msgstr ""

#: of tensorcircuit.backends.pytorch_backend.PyTorchBackend:1
msgid ""
"see the original backend API at `pytorch backend "
"<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/pytorch/pytorch_backend.py>`_"
msgstr ""

#: of tensorcircuit.backends.pytorch_backend.PyTorchBackend:4
msgid ""
"Note the functionality provided by pytorch backend is incomplete, it "
"currenly lacks native efficicent jit and vmap support."
msgstr ""

#: ../../source/modules.rst:49
msgid "tensorcircuit.backends.tensorflow_backend module"
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend:1
msgid "backend magic inherited from tensornetwork: tensorflow backend"
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend:1
msgid ""
"Bases: "
":class:`tensornetwork.backends.tensorflow.tensorflow_backend.TensorFlowBackend`"
msgstr ""

#: of tensorcircuit.backends.tensorflow_backend.TensorFlowBackend:1
msgid ""
"see the original backend API at `tensorflow backend "
"<https://github.com/google/TensorNetwork/blob/master/tensornetwork/backends/tensorflow/tensorflow_backend.py>`_"
msgstr ""

#: ../../source/modules.rst:57
msgid "tensorcircuit.channels module"
msgstr ""

#: of tensorcircuit.channels:1
msgid "some common noise quantum channels"
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:1
msgid ""
"Return an amplitude damping channel. Notice: Amplitude damping "
"corrspondings to p = 1."
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:4
msgid ""
"\\sqrt{p}\n"
"\\begin{bmatrix}\n"
"    1 & 0\\\\\n"
"    0 & \\sqrt{1-\\gamma}\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{p}\n"
"\\begin{bmatrix}\n"
"    0 & \\sqrt{\\gamma}\\\\\n"
"    0 & 0\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{1-p}\n"
"\\begin{bmatrix}\n"
"    \\sqrt{1-\\gamma} & 0\\\\\n"
"    0 & 1\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{1-p}\n"
"\\begin{bmatrix}\n"
"    0 & 0\\\\\n"
"    \\sqrt{\\gamma} & 0\\\\\n"
"\\end{bmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:26
#: tensorcircuit.channels.depolarizingchannel:25
#: tensorcircuit.channels.phasedampingchannel:13
#: tensorcircuit.channels.resetchannel:13
#: tensorcircuit.circuit.Circuit.expectation:3
#: tensorcircuit.circuit.Circuit.measure:3
#: tensorcircuit.cons.set_tensornetwork_backend:5 tensorcircuit.gates.bmatrix:3
#: tensorcircuit.gates.matrix_for_gate:3 tensorcircuit.gates.num_to_tensor:3
#: tensorcircuit.keras.load_func:4 tensorcircuit.keras.save_func:3
#: tensorcircuit.quantum.measurement_counts:4
#: tensorcircuit.simplify.infer_new_shape:3
#: tensorcircuit.utils.return_partial:4 tensorcircuit.vis.qir2tex:1
#: tensorcircuit.vis.render_pdf:5
msgid "Example:"
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:31
msgid "the damping parameter of amplitude (:math:`\\gamma`)"
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:33
msgid ":math:`p`"
msgstr ""

#: of tensorcircuit.channels.amplitudedampingchannel:35
msgid "An amplitude damping channel with given :math:`\\gamma` and :math:`p`"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:1
msgid "Return a Depolarizing Channel"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:3
msgid ""
"\\sqrt{1-p_x-p_y-p_z}\n"
"\\begin{bmatrix}\n"
"    1 & 0\\\\\n"
"    0 & 1\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{p_x}\n"
"\\begin{bmatrix}\n"
"    0 & 1\\\\\n"
"    1 & 0\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{p_y}\n"
"\\begin{bmatrix}\n"
"    0 & -1j\\\\\n"
"    1j & 0\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\sqrt{p_z}\n"
"\\begin{bmatrix}\n"
"    1 & 0\\\\\n"
"    0 & -1\\\\\n"
"\\end{bmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:30
msgid ":math:`p_x`"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:32
msgid ":math:`p_y`"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:34
msgid ":math:`p_z`"
msgstr ""

#: of tensorcircuit.channels.depolarizingchannel:36
msgid "Sequences of Gates"
msgstr ""

#: of tensorcircuit.channels.kraus_to_super_gate:1
msgid "Convert Kraus operators to one Tensor (as one Super Gate)."
msgstr ""

#: of tensorcircuit.channels.kraus_to_super_gate:3
msgid ""
"\\sum_{k}^{} K_k \\otimes K_k^{\\dagger}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.kraus_to_super_gate:6
msgid "A sequence of Gate"
msgstr ""

#: of tensorcircuit.channels.kraus_to_super_gate:8
msgid "The corresponding Tensor of the list of Kraus operators"
msgstr ""

#: of tensorcircuit.channels.phasedampingchannel:1
msgid "Return a phase damping channel with given :math:`\\gamma`"
msgstr ""

#: of tensorcircuit.channels.phasedampingchannel:3
msgid ""
"\\begin{bmatrix}\n"
"    1 & 0\\\\\n"
"    0 & \\sqrt{1-\\gamma}\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\begin{bmatrix}\n"
"    0 & 0\\\\\n"
"    0 & \\sqrt{\\gamma}\\\\\n"
"\\end{bmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.phasedampingchannel:18
msgid "The damping parameter of phase (:math:`\\gamma`)"
msgstr ""

#: of tensorcircuit.channels.phasedampingchannel:20
msgid "A phase damping channel with given :math:`\\gamma`"
msgstr ""

#: of tensorcircuit.channels.resetchannel:1
#: tensorcircuit.channels.resetchannel:18
msgid "Reset channel"
msgstr ""

#: of tensorcircuit.channels.resetchannel:3
msgid ""
"\\begin{bmatrix}\n"
"    1 & 0\\\\\n"
"    0 & 0\\\\\n"
"\\end{bmatrix}\\qquad\n"
"\\begin{bmatrix}\n"
"    0 & 1\\\\\n"
"    0 & 0\\\\\n"
"\\end{bmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.single_qubit_kraus_identity_check:1
msgid "Check identity of a single qubit Kraus operators."
msgstr ""

#: of tensorcircuit.channels.single_qubit_kraus_identity_check:3
msgid "Examples:"
msgstr ""

#: of tensorcircuit.channels.single_qubit_kraus_identity_check:8
msgid ""
"\\sum_{k}^{} K_k^{\\dagger} K_k = I\n"
"\n"
msgstr ""

#: of tensorcircuit.channels.single_qubit_kraus_identity_check:11
msgid "List of Kraus operators."
msgstr ""

#: ../../source/modules.rst:65
msgid "tensorcircuit.circuit module"
msgstr ""

#: of tensorcircuit.circuit:1
msgid "quantum circuit: state simulator"
msgstr ""

#: of tensorcircuit.circuit.Circuit:1
msgid "``Circuit`` class. Simple usage demo below."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply any gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:3
msgid "Qubit number than the gate applies on."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:5
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:5
msgid "Parameters for the gate"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply cnot gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & "
"1.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid "Qubit number than the gate applies on. The matrix for the gate is"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j\\\\    "
"0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply cr gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply crx gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply cry gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply crz gate with parameters on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply cy gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & "
"-0.-1.j\\\\    0.+0.j & 0.+0.j & 0.+1.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & -0.-1.j\\\\"
"    0.+0.j & 0.+0.j & 0.+1.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply cz gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & -1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & -1.+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply exp gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply exp1 gate with parameters on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply h gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    0.70710677+0.j & 0.70710677+0.j\\\\    "
"0.70710677+0.j & -0.70710677+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    0.70710677+0.j & 0.70710677+0.j\\\\    0.70710677+0.j"
" & -0.70710677+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply i gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j "
"\\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply iswap gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.000000e+00+0.j & 0.000000e+00+0.j & "
"0.000000e+00+0.j & 0.000000e+00+0.j\\\\    0.000000e+00+0.j & "
"6.123234e-17+0.j & 0.000000e+00+1.j & 0.000000e+00+0.j\\\\    "
"0.000000e+00+0.j & 0.000000e+00+1.j & 6.123234e-17+0.j & "
"0.000000e+00+0.j\\\\    0.000000e+00+0.j & 0.000000e+00+0.j & "
"0.000000e+00+0.j & 1.000000e+00+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1.000000e+00+0.j & 0.000000e+00+0.j & "
"0.000000e+00+0.j & 0.000000e+00+0.j\\\\    0.000000e+00+0.j & "
"6.123234e-17+0.j & 0.000000e+00+1.j & 0.000000e+00+0.j\\\\    "
"0.000000e+00+0.j & 0.000000e+00+1.j & 6.123234e-17+0.j & "
"0.000000e+00+0.j\\\\    0.000000e+00+0.j & 0.000000e+00+0.j & "
"0.000000e+00+0.j & 1.000000e+00+0.j \\end{bmatrix}"
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply r gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply rx gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply ry gate with parameters on the circuit."
msgstr ""

#: of
#: tensorcircuit.circuit.Circuit.apply_general_variable_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_variable_gate_delayed.<locals>.apply:1
msgid "Apply rz gate with parameters on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply s gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 0.+1.j "
"\\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 0.+1.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply sd gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.-0.j & 0.-0.j\\\\    0.-0.j & 0.-1.j "
"\\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
msgid "\\begin{bmatrix}    1.-0.j & 0.-0.j\\\\    0.-0.j & 0.-1.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply swap gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"0.+0.j & 1.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply t gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1. & +0.j & 0. & +0.j\\\\    0. & +0.j "
"& 0.70710677+0.70710677j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1. & +0.j & 0. & +0.j\\\\    0. & +0.j & "
"0.70710677+0.70710677j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply td gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1. & -0.j & 0. & -0.j\\\\    0. & -0.j "
"& 0.70710677-0.70710677j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1. & -0.j & 0. & -0.j\\\\    0. & -0.j & "
"0.70710677-0.70710677j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply toffoli gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\"
"    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & "
"0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 1.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 1.+0.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j &"
" 0.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & 0.+0.j & "
"1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & 0.+0.j & "
"0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j & 0.+0.j\\\\    0.+0.j & "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j & 0.+0.j\\\\    "
"0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j\\\\"
"    0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 0.+0.j & 1.+0.j & 0.+0.j"
" \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply wroot gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    0.70710677+0.j & -0.5 & -0.5j\\\\    "
"0.5 & -0.5j & 0.70710677+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid ""
"\\begin{bmatrix}    0.70710677+0.j & -0.5 & -0.5j\\\\    0.5 & -0.5j & "
"0.70710677+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply x gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    0.+0.j & 1.+0.j\\\\    1.+0.j & 0.+0.j "
"\\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid "\\begin{bmatrix}    0.+0.j & 1.+0.j\\\\    1.+0.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply y gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    0.+0.j & -0.-1.j\\\\    0.+1.j & 0.+0.j"
" \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid "\\begin{bmatrix}    0.+0.j & -0.-1.j\\\\    0.+1.j & 0.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:1
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:1
msgid "Apply z gate on the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:3
msgid ""
"Qubit number than the gate applies on. The matrix for the gate is  .. "
"math::        \\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & -1.+0.j"
" \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.apply_general_gate_delayed.<locals>.apply:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate_delayed.<locals>.apply:6
msgid "\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & -1.+0.j \\end{bmatrix}"
msgstr ""

#: of tensorcircuit.circuit.Circuit.__init__:1
msgid "Circuit object based on state simulator."
msgstr ""

#: of tensorcircuit.circuit.Circuit.__init__:3
#: tensorcircuit.mpscircuit.MPSCircuit.__init__:3
msgid "The number of qubits in the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.__init__:5
msgid ""
"If not None, the initial state of the circuit is taken as ``inputs`` "
"instead of :math:`\\vert 0\\rangle^n` qubits, defaults to None"
msgstr ""

#: of tensorcircuit.circuit.Circuit.__init__:8
#: tensorcircuit.circuit.Circuit.replace_mps_inputs:3
msgid "(Nodes, dangling Edges) for a MPS like initial wavefunction"
msgstr ""

#: of tensorcircuit.circuit.Circuit.__init__:10
msgid ""
"dict if two qubit gate is ready for split, including parameters for at "
"least one of ``max_singular_values`` and ``max_truncation_err``."
msgstr ""

#: of tensorcircuit.circuit.Circuit.general_kraus:1
msgid ""
"Monte Carlo trajectory simulation of general Kraus channel whose Kraus "
"operators cannot be amplified to unitary operators. For unitary operators"
" composed Kraus channel, :py:meth:`unitary_kraus` is much faster."
msgstr ""

#: of tensorcircuit.circuit.Circuit.general_kraus:5
msgid ""
"This function is jittable in theory. But only jax+GPU combination is "
"recommended for jit since the graph building time is too long for other "
"backend options; though the running time of the function is very fast for"
" every case."
msgstr ""

#: of tensorcircuit.circuit.Circuit.general_kraus:9
msgid "list of ``tn.Node`` for Kraus operators"
msgstr ""

#: of tensorcircuit.circuit.Circuit.general_kraus:11
msgid "the qubits index that Kraus channel is applied on"
msgstr ""

#: of tensorcircuit.circuit.Circuit.general_kraus:13
msgid ""
"random tensor between 0 or 1, defaults to be None, the random number will"
" be generated automatically"
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation:1
msgid "Compute expectation of corresponding operators"
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation:10
msgid ""
"operator and its position on the circuit, eg. ``(tc.gates.z(), [1, ]), "
"(tc.gates.x(), [2, ])`` is for operator :math:`Z_1X_2`"
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation:13
msgid ""
"if True, then the wavefunction tensor is cached for further expectation "
"evaluation, defaults to True"
msgstr ""

#: of tensorcircuit.circuit.Circuit.expectation:17
msgid "Tensor with one element"
msgstr ""

#: of tensorcircuit.circuit.Circuit.is_valid:1
msgid "[WIP], check whether the circuit is legal."
msgstr ""

#: of tensorcircuit.circuit.Circuit.is_valid:3
msgid "the bool indicating whether the circuit is legal"
msgstr ""

#: of tensorcircuit.circuit.Circuit.measure:1
msgid "Take measurement to the given quantum lines."
msgstr ""

#: of tensorcircuit.circuit.Circuit.measure:16
#: tensorcircuit.circuit.Circuit.measure_jit:1
msgid "measure on which quantum line"
msgstr ""

#: of tensorcircuit.circuit.Circuit.measure:17
#: tensorcircuit.circuit.Circuit.measure_jit:3
#: tensorcircuit.mpscircuit.MPSCircuit.measure:2
msgid "if true, theoretical probability is also returned"
msgstr ""

#: of tensorcircuit.circuit.Circuit.mid_measurement:1
msgid ""
"Middle measurement in z-basis on the circuit, note the wavefunction "
"output is not normalized with ``mid_measurement`` involved, one should "
"normalize the state manually if needed."
msgstr ""

#: of tensorcircuit.circuit.Circuit.mid_measurement:4
msgid "the index of qubit that the Z direction postselection applied on"
msgstr ""

#: of tensorcircuit.circuit.Circuit.mid_measurement:6
msgid "0 for spin up, 1 for spin down, defaults to be 0"
msgstr ""

#: of tensorcircuit.circuit.Circuit.perfect_sampling:1
msgid "Reference: arXiv:1201.3974."
msgstr ""

#: of tensorcircuit.circuit.Circuit.perfect_sampling:3
msgid "sampled bit string and the corresponding theoretical probability"
msgstr ""

#: of tensorcircuit.circuit.Circuit.replace_inputs:1
msgid "Replace the input state with the circuit structure unchanged."
msgstr ""

#: of tensorcircuit.circuit.Circuit.replace_inputs:3
msgid "Input wavefunction."
msgstr ""

#: of tensorcircuit.circuit.Circuit.replace_mps_inputs:1
msgid ""
"Replace the input state in MPS representation while keep the circuit "
"structure unchanged."
msgstr ""

#: of tensorcircuit.circuit.Circuit.wavefunction:1
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction:1
msgid "Compute the output wavefunction from the circuit."
msgstr ""

#: of tensorcircuit.circuit.Circuit.wavefunction:3
#: tensorcircuit.mpscircuit.MPSCircuit.wavefunction:3
msgid "the str indicating the form of the output wavefunction"
msgstr ""

#: of tensorcircuit.circuit.Circuit.wavefunction:5
msgid "Tensor with the corresponding shape"
msgstr ""

#: of tensorcircuit.circuit.expectation:1
msgid "Compute :math:`\\langle bra\\vert ops \\vert ket\\rangle`"
msgstr ""

#: of tensorcircuit.circuit.expectation:3
msgid "Example 1 (:math:`bra` is same as :math:`ket`)"
msgstr ""

#: of tensorcircuit.circuit.expectation:24
msgid "Example 2 (:math:`bra` is different from :math:`ket`)"
msgstr ""

#: of tensorcircuit.circuit.expectation:44
msgid "[description], defaults to None, which is the same as ``ket``"
msgstr ""

#: of tensorcircuit.circuit.expectation:46
#: tensorcircuit.quantum.generate_local_hamiltonian:6
#: tensorcircuit.templates.graphs.Line1D:5
msgid "[description], defaults to True"
msgstr ""

#: of tensorcircuit.circuit.expectation:48 tensorcircuit.vis.render_pdf:23
msgid "[description], defaults to False"
msgstr ""

#: of tensorcircuit.circuit.to_graphviz:1
msgid ""
"Not an ideal visualization for quantum circuit, but reserve here as a "
"general approch to show tensornetwork [Deperacted, use ``qir2tex "
"instead``]"
msgstr ""

#: ../../source/modules.rst:74
msgid "tensorcircuit.cons module"
msgstr ""

#: of tensorcircuit.cons:1
msgid "some constants and setups"
msgstr ""

#: of tensorcircuit.cons.get_contractor:1 tensorcircuit.cons.set_contractor:1
msgid ""
"To set runtime contractor of the tensornetwork for a better contraction "
"path."
msgstr ""

#: of tensorcircuit.cons.get_contractor:3 tensorcircuit.cons.set_contractor:3
msgid ""
"\"auto\", \"greedy\", \"branch\", \"plain\", \"tng\", \"custom\", "
"\"custom_stateful\". defaults to None (\"auto\")"
msgstr ""

#: of tensorcircuit.cons.get_contractor:5 tensorcircuit.cons.set_contractor:5
msgid "Valid for \"custom\" or \"custom_stateful\" as method, defaults to None"
msgstr ""

#: of tensorcircuit.cons.get_contractor:7 tensorcircuit.cons.set_contractor:7
msgid ""
"It is not very useful, as ``memory_limit`` leads to ``branch`` "
"contraction instead of ``greedy`` which is rather slow, defaults to None"
msgstr ""

#: of tensorcircuit.cons.get_contractor:10 tensorcircuit.cons.set_contractor:10
msgid "Tensornetwork version is too low to support some of the contractors."
msgstr ""

#: of tensorcircuit.cons.get_contractor:11 tensorcircuit.cons.set_contractor:11
msgid "Unknown method options."
msgstr ""

#: of tensorcircuit.cons.get_contractor:12 tensorcircuit.cons.set_contractor:12
msgid "The new tensornetwork with its contractor set."
msgstr ""

#: of tensorcircuit.cons.get_dtype:1 tensorcircuit.cons.set_dtype:1
msgid "To set the runtime numerical dtype of tensors."
msgstr ""

#: of tensorcircuit.cons.get_dtype:3 tensorcircuit.cons.set_dtype:3
msgid ""
"\"complex64\" or \"complex128\", defaults to None, which is equivalent to"
" \"complex64\"."
msgstr ""

#: of tensorcircuit.cons.plain_contractor:1
msgid "The naive statevector simulator contraction path."
msgstr ""

#: of tensorcircuit.cons.plain_contractor:3
msgid "The list of ``tn.Node``."
msgstr ""

#: of tensorcircuit.cons.plain_contractor:5
msgid "The list of dangling node edges, defaults to None."
msgstr ""

#: of tensorcircuit.cons.plain_contractor:7
msgid "The ``tn.Node`` after contraction"
msgstr ""

#: of tensorcircuit.cons.set_tensornetwork_backend:1
msgid "To set the runtime backend of tensorcircuit."
msgstr ""

#: of tensorcircuit.cons.set_tensornetwork_backend:3
msgid ""
"Note: ``tc.set_backend`` and ``tc.cons.set_tensornetwork_backend`` are "
"the same."
msgstr ""

#: of tensorcircuit.cons.set_tensornetwork_backend:27
msgid ""
"\"numpy\", \"tensorflow\", \"jax\", \"pytorch\". defaults to None, which "
"gives the same behavior as "
"``tensornetwork.backend_contextmanager.get_default_backend()``."
msgstr ""

#: of tensorcircuit.cons.set_tensornetwork_backend:30
msgid "Whether the object should be set as global."
msgstr ""

#: ../../source/modules.rst:82
msgid "tensorcircuit.densitymatrix module"
msgstr ""

#: of tensorcircuit.densitymatrix:1
msgid "quantum circuit class but with density matrix simulator"
msgstr ""

#: ../../source/modules.rst:90
msgid "tensorcircuit.densitymatrix2 module"
msgstr ""

#: of tensorcircuit.densitymatrix2:1
msgid "quantum circuit class but with density matrix simulator: v2"
msgstr ""

#: of tensorcircuit.densitymatrix2.DMCircuit2:1
msgid "Bases: :class:`tensorcircuit.densitymatrix.DMCircuit`"
msgstr ""

#: ../../source/modules.rst:98
msgid "tensorcircuit.experimental module"
msgstr ""

#: of tensorcircuit.experimental:1
msgid "experimental features"
msgstr ""

#: ../../source/modules.rst:107
msgid "tensorcircuit.gates module"
msgstr ""

#: of tensorcircuit.gates:1
msgid ""
"declarations of single-qubit and two-qubit gates and their corresponding "
"matrix"
msgstr ""

#: of tensorcircuit.gates.Gate:1
msgid "Bases: :class:`tensornetwork.network_components.Node`"
msgstr ""

#: of tensorcircuit.gates.Gate:1
msgid "Wrapper of tn.Node, quantum gate"
msgstr ""

#: of tensorcircuit.gates.GateVF:1
msgid "Bases: :class:`tensorcircuit.gates.GateF`"
msgstr ""

#: of tensorcircuit.gates.any_gate:1
msgid "Note one should provide the gate with properly reshaped."
msgstr ""

#: of tensorcircuit.gates.any_gate:3
msgid "corresponding gate"
msgstr ""

#: of tensorcircuit.gates.any_gate:5
msgid "The name of the gate."
msgstr ""

#: of tensorcircuit.gates.any_gate:7
msgid "the resulted gate"
msgstr ""

#: of tensorcircuit.gates.num_to_tensor:1
msgid "Convert the inputs to Tensor with specified dtype."
msgstr ""

#: of tensorcircuit.gates.num_to_tensor:35
msgid "inputs"
msgstr ""

#: of tensorcircuit.gates.num_to_tensor:37
msgid "dtype of the output Tensors"
msgstr ""

#: of tensorcircuit.gates.num_to_tensor:39
msgid "List of Tensors"
msgstr ""

#: of tensorcircuit.gates.bmatrix:1
msgid "Returns a LaTeX bmatrix."
msgstr ""

#: of tensorcircuit.gates.bmatrix:13
msgid "Formatted Display:"
msgstr ""

#: of tensorcircuit.gates.bmatrix:15
msgid ""
"\\begin{bmatrix}    1.+0.j & 0.+0.j\\\\    0.+0.j & 1.+0.j \\end{bmatrix}"
"\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.bmatrix:18
msgid "2D numpy array"
msgstr ""

#: of tensorcircuit.gates.bmatrix:20
msgid "ValueError(\"bmatrix can at most display two dimensions\")"
msgstr ""

#: of tensorcircuit.gates.bmatrix:21
msgid "latex str for bmatrix of array a"
msgstr ""

#: of tensorcircuit.gates.cr_gate:1
msgid ""
"Controlled rotation gate, when the control bit is 1, `rgate` is applied "
"on the target gate."
msgstr ""

#: of tensorcircuit.gates.cr_gate:3 tensorcircuit.gates.cr_gate:5
#: tensorcircuit.gates.cr_gate:7 tensorcircuit.gates.exponential_gate:8
#: tensorcircuit.gates.exponential_gate_unity:9
#: tensorcircuit.gates.iswap_gate:12 tensorcircuit.gates.r_gate:12
#: tensorcircuit.gates.r_gate:14 tensorcircuit.gates.r_gate:16
#: tensorcircuit.gates.rgate_theoretical:12
#: tensorcircuit.gates.rgate_theoretical:14
#: tensorcircuit.gates.rgate_theoretical:16 tensorcircuit.gates.rx_gate:6
#: tensorcircuit.gates.ry_gate:6 tensorcircuit.gates.rz_gate:6
msgid "angle in radians"
msgstr ""

#: of tensorcircuit.gates.cr_gate:10
msgid "CR Gate"
msgstr ""

#: of tensorcircuit.gates.exponential_gate_unity:1
msgid ""
"Faster exponential gate, directly implemented based on RHS, only work "
"when: :math:`U^2` is identity matrix."
msgstr ""

#: of tensorcircuit.gates.exponential_gate_unity:3
msgid ""
"\\rm{exp}(U) &= e^{-i \\theta U} \\\\\n"
"        &= \\cos(\\theta) I - j \\sin(\\theta) U \\\\\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.exponential_gate:6
#: tensorcircuit.gates.exponential_gate_unity:7
msgid "input unitary (U)"
msgstr ""

#: of tensorcircuit.gates.exponential_gate:10
#: tensorcircuit.gates.exponential_gate_unity:11
msgid "suffix of Gate name"
msgstr ""

#: of tensorcircuit.gates.exponential_gate:11
#: tensorcircuit.gates.exponential_gate_unity:13
msgid "Exponential Gate"
msgstr ""

#: of tensorcircuit.gates.exponential_gate:1
msgid "Exponential gate."
msgstr ""

#: of tensorcircuit.gates.exponential_gate:3
msgid ""
"\\rm{exp}(U) = e^{-i \\theta U}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.iswap_gate:1
msgid "iSwap gate."
msgstr ""

#: of tensorcircuit.gates.iswap_gate:3
msgid ""
"iSwap(\\theta) =\n"
"\\begin{pmatrix}\n"
"    1 & 0 & 0 & 0\\\\\n"
"    0 & \\cos(\\frac{\\pi}{2} \\theta ) & j \\sin(\\frac{\\pi}{2} \\theta"
" ) & 0\\\\\n"
"    0 & j \\sin(\\frac{\\pi}{2} \\theta ) & \\cos(\\frac{\\pi}{2} \\theta"
" ) & 0\\\\\n"
"    0 & 0 & 0 & 1\\\\\n"
"\\end{pmatrix}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.iswap_gate:14
msgid "iSwap Gate"
msgstr ""

#: of tensorcircuit.gates.matrix_for_gate:1
msgid "Convert Gate to Tensor."
msgstr ""

#: of tensorcircuit.gates.matrix_for_gate:10
msgid "input Gate"
msgstr ""

#: of tensorcircuit.gates.matrix_for_gate:12
msgid "corresponding Tensor"
msgstr ""

#: of tensorcircuit.gates.meta_gate:1
msgid ""
"Inner helper function to generate gate functions, such as ``z()`` from "
"``_z_matrix``"
msgstr ""

#: of tensorcircuit.gates.r_gate:1
msgid "General single qubit rotation gate"
msgstr ""

#: of tensorcircuit.gates.r_gate:3
msgid ""
"R(\\theta, \\phi, \\alpha) = i \\cos(\\theta) I\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.r_gate:5
msgid ""
"- i \\cos(\\phi) \\sin(\\alpha) \\sin(\\theta) X\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.r_gate:7
msgid ""
"- i \\sin(\\phi) \\sin(\\alpha) \\sin(\\theta) Y\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.r_gate:9
msgid ""
"- i \\sin(\\theta) \\cos(\\alpha) Z\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.r_gate:19
msgid "R Gate"
msgstr ""

#: of tensorcircuit.gates.random_single_qubit_gate:1
msgid "Random single qubit gate described in https://arxiv.org/abs/2002.07730."
msgstr ""

#: of tensorcircuit.gates.random_single_qubit_gate:3
msgid "A random single qubit gate"
msgstr ""

#: of tensorcircuit.gates.random_two_qubit_gate:1
msgid "Returns a random two-qubit gate."
msgstr ""

#: of tensorcircuit.gates.random_two_qubit_gate:3
msgid "a random two-qubit gate"
msgstr ""

#: of tensorcircuit.gates.rgate_theoretical:1
msgid ""
"Rotation gate, which is in matrix exponential form, shall give the same "
"result as `rgate`."
msgstr ""

#: of tensorcircuit.gates.rgate_theoretical:3
msgid ""
"mx = \\sin(\\alpha) \\cos(\\phi) X\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.rgate_theoretical:5
msgid ""
"my = \\sin(\\alpha) \\sin(\\phi) Y\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.rgate_theoretical:7
msgid ""
"mz = \\cos(\\alpha) Z\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.rgate_theoretical:9
msgid ""
"R(\\theta, \\alpha, \\phi) = e^{-i\\theta (mx+my+mz)}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.rgate_theoretical:18
msgid "Rotation Gate"
msgstr ""

#: of tensorcircuit.gates.rx_gate:1
msgid "Rotation gate along X axis."
msgstr ""

#: of tensorcircuit.gates.rx_gate:3
msgid ""
"RX(\\theta) = e^{-i\\frac{\\theta}{2}X}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.rx_gate:8
msgid "RX Gate"
msgstr ""

#: of tensorcircuit.gates.ry_gate:1
msgid "Rotation gate along Y axis."
msgstr ""

#: of tensorcircuit.gates.ry_gate:3
msgid ""
"RY(\\theta) = e^{-i\\frac{\\theta}{2}Y}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.ry_gate:8
msgid "RY Gate"
msgstr ""

#: of tensorcircuit.gates.rz_gate:1
msgid "Rotation gate along Z axis."
msgstr ""

#: of tensorcircuit.gates.rz_gate:3
msgid ""
"RZ(\\theta) = e^{-i\\frac{\\theta}{2}Z}\n"
"\n"
msgstr ""

#: of tensorcircuit.gates.rz_gate:8
msgid "RZ Gate"
msgstr ""

#: ../../source/modules.rst:115
msgid "tensorcircuit.interfaces module"
msgstr ""

#: of tensorcircuit.interfaces:1
msgid "interfaces bridging different backends"
msgstr ""

#: ../../source/modules.rst:123
msgid "tensorcircuit.keras module"
msgstr ""

#: of tensorcircuit.keras:1
msgid "keras layer for tc quantum function"
msgstr ""

#: of tensorcircuit.keras.QuantumLayer:1
msgid "Bases: :class:`keras.engine.base_layer.Layer`"
msgstr ""

#: of tensorcircuit.keras.QuantumLayer.__init__:1
msgid ""
"`QuantumLayer` wraps the quantum function `f` as a `keras.Layer` so that "
"tensorcircuit is better integrated with tensorflow."
msgstr ""

#: of tensorcircuit.keras.QuantumLayer.__init__:8
msgid "[description], defaults to \"glorot_uniform\""
msgstr ""

#: of tensorcircuit.keras.QuantumLayer.build:1
msgid "Creates the variables of the layer (optional, for subclass implementers)."
msgstr ""

#: of tensorcircuit.keras.QuantumLayer.build:3
msgid ""
"This is a method that implementers of subclasses of `Layer` or `Model` "
"can override if they need a state-creation step in-between layer "
"instantiation and layer call."
msgstr ""

#: of tensorcircuit.keras.QuantumLayer.build:7
msgid "This is typically used to create the weights of `Layer` subclasses."
msgstr ""

#: of tensorcircuit.keras.QuantumLayer.build:11
msgid "input_shape: Instance of `TensorShape`, or list of instances of"
msgstr ""

#: of tensorcircuit.keras.QuantumLayer.build:11
msgid ""
"`TensorShape` if the layer expects a list of inputs (one instance per "
"input)."
msgstr ""

#: of tensorcircuit.keras.load_func:1
msgid ""
"Load function from the files in the ``tf.savedmodel`` format. We can load"
" several functions at the same time, as they can be the same function of "
"different input shapes."
msgstr ""

#: of tensorcircuit.keras.load_func:24
msgid ""
"The fallback function when all functions loaded are failed, defaults to "
"None"
msgstr ""

#: of tensorcircuit.keras.load_func:26
msgid ""
"When there is not legal loaded function of the input shape and no "
"fallback callable."
msgstr ""

#: of tensorcircuit.keras.load_func:27
msgid ""
"A function that tries all loaded function against the input until the "
"first success one."
msgstr ""

#: of tensorcircuit.keras.output_asis_loss:1
msgid "The keras loss function that directly taking the model output as the loss."
msgstr ""

#: of tensorcircuit.keras.save_func:1
msgid "Save tf function in the file (``tf.savedmodel`` format)."
msgstr ""

#: of tensorcircuit.keras.save_func:30
msgid "``tf.function`` ed function with graph building"
msgstr ""

#: of tensorcircuit.keras.save_func:32
msgid "the dir path to save the function"
msgstr ""

#: ../../source/modules.rst:131
msgid "tensorcircuit.mpscircuit module"
msgstr ""

#: of tensorcircuit.mpscircuit:1
msgid "quantum circuit: MPS state simulator"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit:1
msgid "``MPSCircuit`` class. Simple usage demo below."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.__init__:1
msgid "MPSCircuit object based on state simulator."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.__init__:5
msgid ""
"If not None, the initial state of the circuit is taken as ``tensors`` "
"instead of :math:`\\vert 0\\rangle^n` qubits, defaults to None"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.__init__:8
msgid "The center position of MPS, default to 0"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate:1
msgid "Apply a general qubit gate on MPS."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate:4
#: tensorcircuit.mpscircuit.MPSCircuit.apply_double_gate:3
#: tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate:3
msgid "The Gate to be applied"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate:6
msgid "Qubit indices of the gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_general_gate:5
msgid "\"MPS does not support application of gate on > 2 qubits.\""
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate:1
msgid ""
"Apply a double qubit gate on adjacent qubits of Matrix Product States "
"(MPS). Truncation rule is specified by `set_truncation_rule`."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate:6
#: tensorcircuit.mpscircuit.MPSCircuit.apply_double_gate:5
msgid "The first qubit index of the gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate:8
#: tensorcircuit.mpscircuit.MPSCircuit.apply_double_gate:7
msgid "The second qubit index of the gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_adjacent_double_gate:10
msgid "Center position of MPS, default is None"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_double_gate:1
msgid ""
"Apply a double qubit gate on MPS. Truncation rule is specified by "
"`set_truncation_rule`."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_single_gate:1
msgid ""
"Apply a single qubit gate on MPS, and the gate must be unitary; no "
"truncation is needed."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_single_gate:3
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_double_gates:3
msgid "gate to be applied"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.apply_single_gate:5
#: tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate:5
msgid "Qubit index of the gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.conj:1
msgid "Compute the conjugate of the current MPS."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.conj:3
#: tensorcircuit.mpscircuit.MPSCircuit.copy:3
#: tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction:11
msgid "The constructed MPS"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.copy:1
msgid "Copy the current MPS."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.copy_without_tensor:1
msgid "Copy the current MPS without the tensors."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.copy_without_tensor:3
msgid "The contructed MPS"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_double_gates:1
msgid "Compute the expectation of the corresponding double qubit gate."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_double_gates:5
msgid "qubit index of the gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate:1
msgid ""
"Compute the expectation of the corresponding single qubit gate in the "
"form of tensor."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate:3
msgid "Gate to be applied"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_single_gate:7
msgid "The expectation of the corresponding single qubit gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product:1
msgid ""
"Compute the expectation of the direct product of the corresponding two "
"gates."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product:3
msgid "First gate to be applied"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product:5
msgid "Second gate to be applied"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product:7
msgid "Qubit index of the first gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product:9
msgid "Qubit index of the second gate"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.expectation_two_gates_product:11
msgid "The correlation of the corresponding two qubit gates"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction:1
msgid "Construct the MPS from a given wavefunction."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction:3
msgid "The given wavefunction (any shape is OK)"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction:5
#: tensorcircuit.mpscircuit.MPSCircuit.set_truncation_rule:5
#: tensorcircuit.mpscircuit.split_tensor:7
msgid "The maximum number of singular values to keep."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction:7
#: tensorcircuit.mpscircuit.MPSCircuit.set_truncation_rule:7
#: tensorcircuit.mpscircuit.split_tensor:9
msgid "The maximum allowed truncation error."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.from_wavefunction:9
#: tensorcircuit.mpscircuit.MPSCircuit.set_truncation_rule:9
#: tensorcircuit.mpscircuit.split_tensor:11
msgid "Multiply `max_truncation_err` with the largest singular value."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.general_expectation:1
msgid "Compute the expectation of corresponding operators in the form of tensor."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.general_expectation:3
msgid ""
"Operator and its position on the circuit, eg. ``(gates.Z(), [1]), "
"(gates.X(), [2])`` is for operator :math:`Z_1X_2`"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.general_expectation:6
msgid "The expectation of corresponding operators"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.get_norm:1
msgid "Get the normalized Center Position."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.get_norm:3
msgid "Normalized Center Position."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.is_valid:1
msgid "Check whether the circuit is legal."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.is_valid:3
msgid "Whether the circuit is legal."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.measure:1
msgid "integer indicating the measure on which quantum line"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.mid_measurement:1
msgid ""
"Middle measurement in the z-basis on the circuit, note the wavefunction "
"output is not normalized with ``mid_measurement`` involved, one should "
"normalized the state manually if needed."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.mid_measurement:4
msgid "The index of qubit that the Z direction postselection applied on"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.mid_measurement:6
msgid "0 for spin up, 1 for spin down, defaults to 0"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.normalize:1
msgid "Normalize MPS Circuit according to the center position."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.position:1
msgid "Wrapper of tn.FiniteMPS.position. Set orthogonality center."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.position:4
msgid "The orthogonality center"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps:1
msgid "Compute the projection between `other` as bra and `self` as ket."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps:3
msgid "ket of the other MPS, which will be converted to bra automatically"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.proj_with_mps:5
msgid "The projection in form of tensor"
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.set_truncation_rule:1
msgid ""
"Set truncation rules when double qubit gates are applied. If nothing is "
"specified, no truncation will take place and the bond dimension will keep"
" growing. For more details, refer to `split_tensor`."
msgstr ""

#: of tensorcircuit.mpscircuit.MPSCircuit.wavefunction:5
msgid "Tensor with shape [1, -1]"
msgstr ""

#: of tensorcircuit.mpscircuit.split_tensor:1
msgid "Split the tensor by SVD or QR depends on whether a truncation is required."
msgstr ""

#: of tensorcircuit.mpscircuit.split_tensor:3
msgid "The input tensor to split."
msgstr ""

#: of tensorcircuit.mpscircuit.split_tensor:5
msgid "Determine the orthogonal center is on the left tensor or the right tensor."
msgstr ""

#: of tensorcircuit.mpscircuit.split_tensor:13
msgid "Two tensors after splitting"
msgstr ""

#: ../../source/modules.rst:139
msgid "tensorcircuit.quantum module"
msgstr ""

#: of tensorcircuit.quantum:1
msgid "quantum state and operator class backend by tensornetwork"
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector:1 tensorcircuit.quantum.QuScalar:1
#: tensorcircuit.quantum.QuVector:1
msgid "Bases: :class:`tensorcircuit.quantum.QuOperator`"
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector:1
msgid "Represents an adjoint (row) vector via a tensor network."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.__init__:1
msgid ""
"Constructs a new `QuAdjointVector` from a tensor network. This "
"encapsulates an existing tensor network, interpreting it as an adjoint "
"vector (row vector)."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.__init__:5
#: tensorcircuit.quantum.QuOperator.__init__:9
msgid "The edges of the network to be used as the input edges."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.__init__:7
#: tensorcircuit.quantum.QuOperator.__init__:11
#: tensorcircuit.quantum.QuVector.__init__:6
msgid ""
"Nodes used to refer to parts of the tensor network that are not connected"
" to any input or output edges (for example: a scalar factor)."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.__init__:10
#: tensorcircuit.quantum.QuScalar.__init__:7
#: tensorcircuit.quantum.QuVector.__init__:9
msgid "Optional collection of edges to ignore when performing consistency checks."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.from_tensor:1
msgid ""
"Construct a `QuAdjointVector` directly from a single tensor. This first "
"wraps the tensor in a `Node`, then constructs the `QuAdjointVector` from "
"that `Node`."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.from_tensor:4
msgid "The tensor for consturcting an QuAdjointVector."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.from_tensor:6
msgid ""
"Sequence of integer indices specifying the order in which to interpret "
"the axes as subsystems (input edges). If not specified, the axes are "
"taken in ascending order."
msgstr ""

#: of tensorcircuit.quantum.QuAdjointVector.from_tensor:10
msgid "The new construted QuAdjointVector give from the given tensor."
msgstr ""

#: of tensorcircuit.quantum.QuOperator:1
msgid ""
"Represents a linear operator via a tensor network. To interpret a tensor "
"network as a linear operator, some of the dangling edges must be "
"designated as `out_edges` (output edges) and the rest as `in_edges` "
"(input edges). Considered as a matrix, the `out_edges` represent the row "
"index and the `in_edges` represent the column index. The (right) action "
"of the operator on another then consists of connecting the `in_edges` of "
"the first operator to the `out_edges` of the second. Can be used to do "
"simple linear algebra with tensor networks."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.__init__:1
msgid ""
"Creates a new `QuOperator` from a tensor network. This encapsulates an "
"existing tensor network, interpreting it as a linear operator. The "
"network is checked for consistency: All dangling edges must either be in "
"`out_edges`, `in_edges`, or `ignore_edges`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.__init__:7
#: tensorcircuit.quantum.QuVector.__init__:4
msgid "The edges of the network to be used as the output edges."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.__init__:15
msgid ""
"Optional collection of dangling edges to ignore when performing "
"consistency checks."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.__init__:18
msgid ""
"At least one reference node is required to specify a scalar. None "
"provided!"
msgstr ""

#: of tensorcircuit.quantum.QuOperator.adjoint:1
msgid ""
"The adjoint of the operator. This creates a new `QuOperator` with "
"complex-conjugate copies of all tensors in the network and with the input"
" and output edges switched."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.check_network:1
msgid ""
"Check that the network has the expected dimensionality. This checks that "
"all input and output edges are dangling and that there are no other "
"dangling edges (except any specified in `ignore_edges`). If not, an "
"exception is raised."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.contract:1
msgid ""
"Contract the tensor network in place. This modifies the tensor network "
"representation of the operator (or vector, or scalar), reducing it to a "
"single tensor, without changing the value."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.contract:5
msgid "Manually specify the axis ordering of the final tensor."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.contract:7
msgid "The present object."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval:1
msgid ""
"Contracts the tensor network in place and returns the final tensor. Note "
"that this modifies the tensor network representing the operator. The "
"default ordering for the axes of the final tensor is:"
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval:4
msgid "`*out_edges, *in_edges`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval:6
msgid "If there are any \"ignored\" edges, their axes come first:"
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval:6
msgid "`*ignored_edges, *out_edges, *in_edges`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval:8
msgid ""
"Manually specify the axis ordering of the final tensor. The default "
"ordering is determined by `out_edges` and `in_edges` (see above)."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval:11
msgid "Node count '{}' > 1 after contraction!"
msgstr ""

#: of tensorcircuit.quantum.QuOperator.eval:12
msgid "The final tensor representing the operator."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.from_tensor:1
msgid ""
"Construct a `QuOperator` directly from a single tensor. This first wraps "
"the tensor in a `Node`, then constructs the `QuOperator` from that "
"`Node`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.from_tensor:5
msgid "The tensor."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.from_tensor:7
msgid "The axis indices of `tensor` to use as `out_edges`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.from_tensor:9
msgid "The axis indices of `tensor` to use as `in_edges`."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.from_tensor:11
msgid "The new operator."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.nodes:1
msgid "All tensor-network nodes involved in the operator."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.norm:1
msgid ""
"The norm of the operator. This is the 2-norm (also known as the Frobenius"
" or Hilbert-Schmidt norm)."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.partial_trace:1
msgid ""
"The partial trace of the operator. Subsystems to trace out are supplied "
"as indices, so that dangling edges are connected to each other as:"
msgstr ""

#: of tensorcircuit.quantum.QuOperator.partial_trace:4
msgid "`out_edges[i] ^ in_edges[i] for i in subsystems_to_trace_out`"
msgstr ""

#: of tensorcircuit.quantum.QuOperator.partial_trace:5
msgid ""
"This does not modify the original network. The original ordering of the "
"remaining subsystems is maintained."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.partial_trace:8
msgid "Indices of subsystems to trace out."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.partial_trace:10
msgid "A new QuOperator or QuScalar representing the result."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.tensor_product:1
msgid ""
"Tensor product with another operator. Given two operators `A` and `B`, "
"produces a new operator `AB` representing `A` ⊗ `B`. The `out_edges` "
"(`in_edges`) of `AB` is simply the concatenation of the `out_edges` "
"(`in_edges`) of `A.copy()` with that of `B.copy()`: `new_out_edges = "
"[*out_edges_A_copy, *out_edges_B_copy]` `new_in_edges = "
"[*in_edges_A_copy, *in_edges_B_copy]`"
msgstr ""

#: of tensorcircuit.quantum.QuOperator.tensor_product:9
msgid "The other operator (`B`)."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.tensor_product:11
msgid "The result (`AB`)."
msgstr ""

#: of tensorcircuit.quantum.QuOperator.trace:1
msgid "The trace of the operator."
msgstr ""

#: of tensorcircuit.quantum.QuScalar:1
msgid "Represents a scalar via a tensor network."
msgstr ""

#: of tensorcircuit.quantum.QuScalar.__init__:1
msgid ""
"Constructs a new `QuScalar` from a tensor network. This encapsulates an "
"existing tensor network, interpreting it as a scalar."
msgstr ""

#: of tensorcircuit.quantum.QuScalar.__init__:4
msgid ""
"Nodes used to refer to the tensor network (need not be exhaustive - one "
"node from each disconnected subnetwork is sufficient)."
msgstr ""

#: of tensorcircuit.quantum.QuScalar.from_tensor:1
msgid ""
"Construct a `QuScalar` directly from a single tensor. This first wraps "
"the tensor in a `Node`, then constructs the `QuScalar` from that `Node`."
msgstr ""

#: of tensorcircuit.quantum.QuScalar.from_tensor:4
msgid "The tensor for constructing a new QuScalar."
msgstr ""

#: of tensorcircuit.quantum.QuScalar.from_tensor:6
msgid "The new constructed QuScalar from the given tensor."
msgstr ""

#: of tensorcircuit.quantum.QuVector:1
msgid "Represents a (column) vector via a tensor network."
msgstr ""

#: of tensorcircuit.quantum.QuVector.__init__:1
msgid ""
"Constructs a new `QuVector` from a tensor network. This encapsulates an "
"existing tensor network, interpreting it as a (column) vector."
msgstr ""

#: of tensorcircuit.quantum.QuVector.from_tensor:1
msgid ""
"Construct a `QuVector` directly from a single tensor. This first wraps "
"the tensor in a `Node`, then constructs the `QuVector` from that `Node`."
msgstr ""

#: of tensorcircuit.quantum.QuVector.from_tensor:5
msgid "The tensor for constructing a \"QuVector\"."
msgstr ""

#: of tensorcircuit.quantum.QuVector.from_tensor:7
msgid ""
"Sequence of integer indices specifying the order in which to interpret "
"the axes as subsystems (output edges). If not specified, the axes are "
"taken in ascending order."
msgstr ""

#: of tensorcircuit.quantum.QuVector.from_tensor:11
msgid "The new constructed QuVector from the given tensor."
msgstr ""

#: of tensorcircuit.quantum.check_spaces:1
msgid ""
"Check the vector spaces represented by two lists of edges are compatible."
" The number of edges must be the same and the dimensions of each pair of "
"edges must match. Otherwise, an exception is raised. :param edges_1: List"
" of edges representing a many-body Hilbert space. :type edges_1: "
"Sequence[Edge] :param edges_2: List of edges representing a many-body "
"Hilbert space. :type edges_2: Sequence[Edge]"
msgstr ""

#: of tensorcircuit.quantum.check_spaces:9
msgid ""
"Hilbert-space mismatch: \"Cannot connect {} subsystems with {} "
"subsystems\", or \"Input dimension {} != output dimension {}.\""
msgstr ""

#: of tensorcircuit.quantum.eliminate_identities:1
msgid ""
"Eliminates any connected CopyNodes that are identity matrices. This will "
"modify the network represented by `nodes`. Only identities that are "
"connected to other nodes are eliminated."
msgstr ""

#: of tensorcircuit.quantum.eliminate_identities:5
msgid "Collection of nodes to search."
msgstr ""

#: of tensorcircuit.quantum.eliminate_identities:7
msgid ""
"The Dictionary mapping remaining Nodes to any replacements, Dictionary "
"specifying all dangling-edge replacements."
msgstr ""

#: of tensorcircuit.quantum.entropy:1
msgid "Compute the entropy from the given density matrix ``rho``."
msgstr ""

#: of tensorcircuit.quantum.entropy:5
msgid "[description], defaults to 1e-12"
msgstr ""

#: of tensorcircuit.quantum.generate_local_hamiltonian:1
msgid ""
"Note: further jit is recommended, for large Hilbert space, sparse "
"Hamiltonian is recommended"
msgstr ""

#: of tensorcircuit.quantum.identity:1
msgid ""
"Construct a 'QuOperator' representing the identity on a given space. "
"Internally, this is done by constructing 'CopyNode's for each edge, with "
"dimension according to 'space'."
msgstr ""

#: of tensorcircuit.quantum.identity:5
msgid ""
"A sequence of integers for the dimensions of the tensor product factors "
"of the space (the edges in the tensor network)."
msgstr ""

#: of tensorcircuit.quantum.identity:8
msgid "The data type (for conversion to dense)."
msgstr ""

#: of tensorcircuit.quantum.identity:10
msgid "The desired identity operator."
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:1
msgid ""
"Simulate the measuring of each qubit of ``p`` in the computational basis,"
" thus producing output like that of ``qiskit``."
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:14
msgid ""
"The quantum state, assumed to be normalized, as either a ket or density "
"operator."
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:16
msgid "The number of counts to perform."
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:18
msgid ""
"Defaults True. The bool indicating whether the return form is in the form"
" of two array or one of the same length as the ``state`` (if "
"``sparse=False``)."
msgstr ""

#: of tensorcircuit.quantum.measurement_counts:21
msgid "The counts for each bit string measured."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:1
msgid ""
"Constructs an appropriately specialized QuOperator. If there are no "
"edges, creates a QuScalar. If the are only output (input) edges, creates "
"a QuVector (QuAdjointVector). Otherwise creates a QuOperator."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:5
msgid "output edges."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:7
msgid "in edges."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:9
msgid ""
"reference nodes for the tensor network (needed if there is a scalar "
"component)."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:12
msgid "edges to ignore when checking the dimensionality of the tensor network."
msgstr ""

#: of tensorcircuit.quantum.quantum_constructor:15
msgid "The new created QuOperator object."
msgstr ""

#: of tensorcircuit.quantum.reduced_density_matrix:1
msgid "Compute the reduced density matrix from quantum state ``state``."
msgstr ""

#: of tensorcircuit.quantum.trace_product:1
msgid "Compute the trace of several inputs ``o`` as tensor or ``QuOperator``."
msgstr ""

#: of tensorcircuit.quantum.trace_product:3
msgid "\\mathrm{Tr}(\\prod_i O_i)"
msgstr ""

#: of tensorcircuit.quantum.trace_product:7
msgid "a scalar"
msgstr ""

#: ../../source/modules.rst:147
msgid "tensorcircuit.simplify module"
msgstr ""

#: of tensorcircuit.simplify:1
msgid "tensornetwork simplification"
msgstr ""

#: of tensorcircuit.simplify.infer_new_shape:1
msgid ""
"Get the new shape of two nodes, also supporting to return original shapes"
" of two nodes."
msgstr ""

#: of tensorcircuit.simplify.infer_new_shape:13
msgid "node one"
msgstr ""

#: of tensorcircuit.simplify.infer_new_shape:15
msgid "node two"
msgstr ""

#: of tensorcircuit.simplify.infer_new_shape:17
msgid "Whether to include original shape of two nodes, default is True."
msgstr ""

#: of tensorcircuit.simplify.infer_new_shape:19
msgid "The new shape of the two nodes."
msgstr ""

#: of tensorcircuit.simplify.pseudo_contract_between:1
msgid ""
"Contract between Node ``a`` and ``b``, with correct shape only and no "
"calculation"
msgstr ""

#: ../../source/modules.rst:155
msgid "tensorcircuit.templates module"
msgstr ""

#: ../../source/modules.rst:163
msgid "tensorcircuit.templates.blocks module"
msgstr ""

#: of tensorcircuit.templates.blocks:1 tensorcircuit.templates.measurements:1
msgid "shortcuts for measurement patterns on circuit"
msgstr ""

#: ../../source/modules.rst:171
msgid "tensorcircuit.templates.graphs module"
msgstr ""

#: of tensorcircuit.templates.graphs:1
msgid "Some common graphs and lattices"
msgstr ""

#: of tensorcircuit.templates.graphs.Line1D:1
msgid "1D chain with ``n`` sites"
msgstr ""

#: ../../source/modules.rst:179
msgid "tensorcircuit.templates.measurements module"
msgstr ""

#: of tensorcircuit.templates.measurements.any_measurements:1
msgid ""
"This measurements pattern is specifically suitable for vmap. Parameterize"
" the Pauli string to be measured."
msgstr ""

#: of tensorcircuit.templates.measurements.any_measurements:6
msgid ""
"parameter tensors determines what Pauli string to be measured, shape is "
"[nwires, 4] if onehot is False."
msgstr ""

#: of tensorcircuit.templates.measurements.any_measurements:9
msgid ""
"[description], defaults to False. If set to be True, structures will "
"first go through onehot procedure."
msgstr ""

#: of tensorcircuit.templates.measurements.sparse_expectation:5
msgid "COO_sparse_matrix"
msgstr ""

#: of tensorcircuit.templates.measurements.sparse_expectation:7
msgid "a real and scalar tensor of shape []"
msgstr ""

#: ../../source/modules.rst:187
msgid "tensorcircuit.utils module"
msgstr ""

#: of tensorcircuit.utils:1
msgid "some helper functions"
msgstr ""

#: of tensorcircuit.utils.return_partial:1
msgid ""
"Return a callable function for output ith parts of the original output "
"along the first axis. Original output supports List and Tensor."
msgstr ""

#: of tensorcircuit.utils.return_partial:20
msgid "The function to be applied this method"
msgstr ""

#: of tensorcircuit.utils.return_partial:22
msgid "The ith parts of original output along the first axis (axis=0 or dim=0)"
msgstr ""

#: of tensorcircuit.utils.return_partial:24
msgid "The modified callable function"
msgstr ""

#: ../../source/modules.rst:195
msgid "tensorcircuit.vis module"
msgstr ""

#: of tensorcircuit.vis:1
msgid "visualization on circuits"
msgstr ""

#: of tensorcircuit.vis.qir2tex:2
msgid "# TODO(@YHPeter): add examples"
msgstr ""

#: of tensorcircuit.vis.render_pdf:1
msgid ""
"Generate the PDF file with given latex string and filename. Latex command"
" and file path can be specified. When notebook is True, convert the "
"output PDF file to image and return a Image object."
msgstr ""

#: of tensorcircuit.vis.render_pdf:15
msgid "String of latex content"
msgstr ""

#: of tensorcircuit.vis.render_pdf:17
msgid "File name, defaults to random UUID `str(uuid4())`"
msgstr ""

#: of tensorcircuit.vis.render_pdf:19
msgid "Executable Latex command, defaults to `pdflatex`"
msgstr ""

#: of tensorcircuit.vis.render_pdf:21
msgid "File path, defaults to current working place `os.getcwd()`"
msgstr ""

#: of tensorcircuit.vis.render_pdf:25
msgid "if notebook is True, return `Image` object; otherwise return `None`"
msgstr ""

#~ msgid "get the `tc.backend` object"
#~ msgstr ""

#~ msgid "Backend doesn't exist for `backend` argument"
#~ msgstr ""

#~ msgid "the `tc.backend` object that with all registered universal functions"
#~ msgstr ""

#~ msgid ""
#~ "call random normal function with the "
#~ "random state management behind the scene"
#~ msgstr ""

#~ msgid ""
#~ "a jax like split API, but does't"
#~ " split the key generator for other"
#~ " backends. just for a consistent "
#~ "interface of random code, be careful "
#~ "that you know what the function "
#~ "actually does."
#~ msgstr ""

#~ msgid "set random state attached in the backend"
#~ msgstr ""

#~ msgid "uniform random sampler for ``low`` to ``high``"
#~ msgstr ""

#~ msgid "compute expectation of corresponding operators"
#~ msgstr ""

#~ msgid "reference: arXiv:1201.3974."
#~ msgstr ""

#~ msgid "compute the output wavefunction from the circuit"
#~ msgstr ""

#~ msgid ""
#~ "[deprecated] direct manipulate on  "
#~ "``QuOperator`` is suggested compute "
#~ ":math:`\\langle bra\\vert ops \\vert "
#~ "ket\\rangle`"
#~ msgstr ""

#~ msgid ""
#~ "set runtime contractor of the "
#~ "tensornetwork for a better contraction "
#~ "path"
#~ msgstr ""

#~ msgid "valid for \"custom\" or \"custom_stateful\" as method, defaults to None"
#~ msgstr ""

#~ msgid ""
#~ "not very useful, as ``memory_limit`` "
#~ "leads to ``branch`` contraction instead "
#~ "of ``greedy`` which is rather slow, "
#~ "defaults to None"
#~ msgstr ""

#~ msgid "tensornetwork version is too low to support some of the contractors"
#~ msgstr ""

#~ msgid "unknown method options"
#~ msgstr ""

#~ msgid "set the runtime numerical dtype of tensors"
#~ msgstr ""

#~ msgid ""
#~ "\"complex64\" or \"complex128\", defaults to"
#~ " None, which is equivalent to "
#~ "\"complex64\""
#~ msgstr ""

#~ msgid "naive statevector simulator contraction path"
#~ msgstr ""

#~ msgid "list of ``tn.Node``"
#~ msgstr ""

#~ msgid "list of dangling node edges, defaults to None"
#~ msgstr ""

#~ msgid "``tn.Node`` after contraction"
#~ msgstr ""

#~ msgid "set the runtime backend of tensorcircuit"
#~ msgstr ""

#~ msgid ""
#~ "\"numpy\", \"tensorflow\", \"jax\", \"pytorch\". "
#~ "defaults to None, which gives the "
#~ "same behavior as "
#~ "``tensornetwork.backend_contextmanager.get_default_backend()``"
#~ msgstr ""

#~ msgid "Note one should provide the gate with properly reshaped"
#~ msgstr ""

#~ msgid "Returns a LaTeX bmatrix"
#~ msgstr ""

#~ msgid "$\\exp{-i    heta unitary}$"
#~ msgstr ""

#~ msgid ""
#~ "Returns the random single qubit gate "
#~ "described in https://arxiv.org/abs/2002.07730."
#~ msgstr ""

#~ msgid "e^{-        heta/2 i X}"
#~ msgstr ""

#~ msgid "e^{-        heta/2 i Y}"
#~ msgstr ""

#~ msgid "e^{-        heta/2 i Z}"
#~ msgstr ""

#~ msgid ""
#~ "`QuantumLayer` wraps the quantum function "
#~ "`f` as a `keras.Layer`, so that "
#~ "tensorcircuit is better integrated with "
#~ "tensorflow"
#~ msgstr ""

#~ msgid ""
#~ "Load function from the files in "
#~ "``tf.savedmodel`` format. We can load "
#~ "several functions at the same time, "
#~ "as they can be the same function"
#~ " of different input shapes."
#~ msgstr ""

#~ msgid "keras loss function that directly taking the model output at the loss"
#~ msgstr ""

#~ msgid "save tf function in file (``tf.savedmodel`` format)"
#~ msgstr ""

#~ msgid ""
#~ "mps = tc.MPSCircuit(3) mps.H(1) mps.CNOT(0,"
#~ " 1) mps.rx(2, theta=tc.num_to_tensor(1.)) "
#~ "mps.expectation_single_gate(tc.gates.z(), 2)"
#~ msgstr ""

#~ msgid "the center position of MPS, default to 0"
#~ msgstr ""

#~ msgid "qubit indices of the gate"
#~ msgstr ""

#~ msgid ""
#~ "Apply a double qubit gate on "
#~ "adjacent qubits of MPS, truncation rule"
#~ " is speficied by `set_truncation_rule`"
#~ msgstr ""

#~ msgid ""
#~ "Apply a double qubit gate on MPS,"
#~ " truncation rule is speficied by "
#~ "`set_truncation_rule` :param gate: gate to "
#~ "be applied :type gate: Gate :param "
#~ "index1: first qubit index of the "
#~ "gate :type index1: int :param index2:"
#~ " second qubit index of the gate "
#~ ":type index2: int"
#~ msgstr ""

#~ msgid ""
#~ "Apply a single qubit gate on MPS,"
#~ " the gate must be unitary, no "
#~ "truncation is needed :param gate: gate"
#~ " to be applied :type gate: Gate "
#~ ":param index: qubit indices of the "
#~ "gate :type index: int"
#~ msgstr ""

#~ msgid "Get the conjugate of the current MPS"
#~ msgstr ""

#~ msgid "Copy the current MPS"
#~ msgstr ""

#~ msgid "Copy the current MPS without the tensors"
#~ msgstr ""

#~ msgid "Compute expectation of the corresponding single qubit gate"
#~ msgstr ""

#~ msgid "Compute correlation of the corresponding two gates"
#~ msgstr ""

#~ msgid "first gate to be applied"
#~ msgstr ""

#~ msgid "second gate to be applied"
#~ msgstr ""

#~ msgid "qubit index of the first gate"
#~ msgstr ""

#~ msgid "qubit index of the second gate"
#~ msgstr ""

#~ msgid "Construct the MPS from a given wavefunction"
#~ msgstr ""

#~ msgid ""
#~ "operator and its position on the "
#~ "circuit, eg. ``(gates.Z(), [1]), (gates.X(),"
#~ " [2])`` is for operator :math:`Z_1X_2`"
#~ msgstr ""

#~ msgid "check whether the circuit is legal"
#~ msgstr ""

#~ msgid ""
#~ "Wrapper of tn.FiniteMPS.position Set "
#~ "orthogonality center :param site: "
#~ "orthogonality center :type site: int"
#~ msgstr ""

#~ msgid "compute the projection between `other` as bra and `self` as ket"
#~ msgstr ""

#~ msgid ""
#~ "Set truncation rules when double qubit"
#~ " gates are applied. If nothing is "
#~ "specified, no truncation will take place"
#~ " and the bond dimension will keep "
#~ "growing. For more details, refer to "
#~ "`split_tensor`"
#~ msgstr ""

#~ msgid ""
#~ "Split the tensor by SVD or QR "
#~ "depends on whether a truncation is "
#~ "required"
#~ msgstr ""

#~ msgid "two tensors after splitting"
#~ msgstr ""

#~ msgid ""
#~ "Constructs a new `QuAdjointVector` from "
#~ "a tensor network. This encapsulates an"
#~ " existing tensor network, interpreting it"
#~ " as an adjoint vector (row vector)."
#~ " Args:"
#~ msgstr ""

#~ msgid ""
#~ "subsystem_edges: The edges of the "
#~ "network to be used as the input"
#~ " edges. ref_nodes: Nodes used to "
#~ "refer to parts of the tensor "
#~ "network that are"
#~ msgstr ""

#~ msgid ""
#~ "not connected to any input or "
#~ "output edges (for example: a scalar "
#~ "factor)."
#~ msgstr ""

#~ msgid "ignore_edges: Optional collection of edges to ignore when performing"
#~ msgstr ""

#~ msgid "consistency checks."
#~ msgstr ""

#~ msgid ""
#~ "Construct a `QuAdjointVector` directly from"
#~ " a single tensor. This first wraps"
#~ " the tensor in a `Node`, then "
#~ "constructs the `QuAdjointVector` from that "
#~ "`Node`. Args:"
#~ msgstr ""

#~ msgid ""
#~ "tensor: The tensor. subsystem_axes: Sequence"
#~ " of integer indices specifying the "
#~ "order in which"
#~ msgstr ""

#~ msgid ""
#~ "to interpret the axes as subsystems "
#~ "(input edges). If not specified, the "
#~ "axes are taken in ascending order."
#~ msgstr ""

#~ msgid "backend: Optionally specify the backend to use for computations."
#~ msgstr ""

#~ msgid ""
#~ "Creates a new `QuOperator` from a "
#~ "tensor network. This encapsulates an "
#~ "existing tensor network, interpreting it "
#~ "as a linear operator. The network "
#~ "is checked for consistency: All dangling"
#~ " edges must either be in `out_edges`,"
#~ " `in_edges`, or `ignore_edges`. Args:"
#~ msgstr ""

#~ msgid ""
#~ "out_edges: The edges of the network "
#~ "to be used as the output edges."
#~ " in_edges: The edges of the network"
#~ " to be used as the input edges."
#~ " ref_nodes: Nodes used to refer to"
#~ " parts of the tensor network that "
#~ "are"
#~ msgstr ""

#~ msgid "ignore_edges: Optional collection of dangling edges to ignore when"
#~ msgstr ""

#~ msgid "performing consistency checks."
#~ msgstr ""

#~ msgid ""
#~ "Contract the tensor network in place."
#~ " This modifies the tensor network "
#~ "representation of the operator (or "
#~ "vector, or scalar), reducing it to "
#~ "a single tensor, without changing the"
#~ " value. Args:"
#~ msgstr ""

#~ msgid "contractor: A function that performs the contraction. Defaults to"
#~ msgstr ""

#~ msgid ""
#~ "`greedy`, which uses the greedy "
#~ "algorithm from `opt_einsum` to determine "
#~ "a contraction order."
#~ msgstr ""

#~ msgid ""
#~ "final_edge_order: Manually specify the axis"
#~ " ordering of the final tensor."
#~ msgstr ""

#~ msgid ""
#~ "The default ordering is determined by"
#~ " `out_edges` and `in_edges` (see above)."
#~ msgstr ""

#~ msgid ""
#~ "Construct a `QuOperator` directly from a"
#~ " single tensor. This first wraps the"
#~ " tensor in a `Node`, then constructs"
#~ " the `QuOperator` from that `Node`. "
#~ "Args:"
#~ msgstr ""

#~ msgid ""
#~ "tensor: The tensor. out_axes: The axis"
#~ " indices of `tensor` to use as "
#~ "`out_edges`. in_axes: The axis indices "
#~ "of `tensor` to use as `in_edges`. "
#~ "backend: Optionally specify the backend "
#~ "to use for computations."
#~ msgstr ""

#~ msgid ""
#~ "The partial trace of the operator. "
#~ "Subsystems to trace out are supplied "
#~ "as indices, so that dangling edges "
#~ "are connected to eachother as:"
#~ msgstr ""

#~ msgid ""
#~ "This does not modify the original "
#~ "network. The original ordering of the"
#~ " remaining subsystems is maintained. Args:"
#~ msgstr ""

#~ msgid "subsystems_to_trace_out: Indices of subsystems to trace out."
#~ msgstr ""

#~ msgid ""
#~ "Tensor product with another operator. "
#~ "Given two operators `A` and `B`, "
#~ "produces a new operator `AB` "
#~ "representing `A` ⊗ `B`. The `out_edges`"
#~ " (`in_edges`) of `AB` is simply the"
#~ " concatenation of the `out_edges` "
#~ "(`in_edges`) of `A.copy()` with that of"
#~ " `B.copy()`: `new_out_edges = [*out_edges_A_copy,"
#~ " *out_edges_B_copy]` `new_in_edges = "
#~ "[*in_edges_A_copy, *in_edges_B_copy]` Args:"
#~ msgstr ""

#~ msgid "other: The other operator (`B`)."
#~ msgstr ""

#~ msgid ""
#~ "Constructs a new `QuScalar` from a "
#~ "tensor network. This encapsulates an "
#~ "existing tensor network, interpreting it "
#~ "as a scalar. Args:"
#~ msgstr ""

#~ msgid "ref_nodes: Nodes used to refer to the tensor network (need not be"
#~ msgstr ""

#~ msgid ""
#~ "exhaustive - one node from each "
#~ "disconnected subnetwork is sufficient). "
#~ "ignore_edges: Optional collection of edges "
#~ "to ignore when performing consistency "
#~ "checks."
#~ msgstr ""

#~ msgid ""
#~ "Construct a `QuScalar` directly from a"
#~ " single tensor. This first wraps the"
#~ " tensor in a `Node`, then constructs"
#~ " the `QuScalar` from that `Node`. "
#~ "Args:"
#~ msgstr ""

#~ msgid ""
#~ "tensor: The tensor. backend: Optionally "
#~ "specify the backend to use for "
#~ "computations."
#~ msgstr ""

#~ msgid ""
#~ "Constructs a new `QuVector` from a "
#~ "tensor network. This encapsulates an "
#~ "existing tensor network, interpreting it "
#~ "as a (column) vector. Args:"
#~ msgstr ""

#~ msgid ""
#~ "subsystem_edges: The edges of the "
#~ "network to be used as the output"
#~ " edges. ref_nodes: Nodes used to "
#~ "refer to parts of the tensor "
#~ "network that are"
#~ msgstr ""

#~ msgid ""
#~ "Construct a `QuVector` directly from a"
#~ " single tensor. This first wraps the"
#~ " tensor in a `Node`, then constructs"
#~ " the `QuVector` from that `Node`. "
#~ "Args:"
#~ msgstr ""

#~ msgid ""
#~ "to interpret the axes as subsystems "
#~ "(output edges). If not specified, the"
#~ " axes are taken in ascending order."
#~ msgstr ""

#~ msgid ""
#~ "Check the vector spaces represented by"
#~ " two lists of edges are compatible."
#~ " The number of edges must be "
#~ "the same and the dimensions of "
#~ "each pair of edges must match. "
#~ "Otherwise, an exception is raised. Args:"
#~ msgstr ""

#~ msgid ""
#~ "edges_1: List of edges representing a"
#~ " many-body Hilbert space. edges_2: "
#~ "List of edges representing a many-"
#~ "body Hilbert space."
#~ msgstr ""

#~ msgid ""
#~ "Eliminates any connected CopyNodes that "
#~ "are identity matrices. This will modify"
#~ " the network represented by `nodes`. "
#~ "Only identities that are connected to"
#~ " other nodes are eliminated. Args:"
#~ msgstr ""

#~ msgid "nodes: Collection of nodes to search."
#~ msgstr ""

#~ msgid ""
#~ "nodes_dict: Dictionary mapping remaining Nodes"
#~ " to any replacements. dangling_edges_dict: "
#~ "Dictionary specifying all dangling-edge "
#~ "replacements."
#~ msgstr ""

#~ msgid ""
#~ "Construct a `QuOperator` representing the "
#~ "identity on a given space. Internally,"
#~ " this is done by constructing "
#~ "`CopyNode`s for each edge, with "
#~ "dimension according to `space`. Args:"
#~ msgstr ""

#~ msgid "space: A sequence of integers for the dimensions of the tensor product"
#~ msgstr ""

#~ msgid "factors of the space (the edges in the tensor network)."
#~ msgstr ""

#~ msgid ""
#~ "backend: Optionally specify the backend "
#~ "to use for computations. dtype: The "
#~ "data type (for conversion to dense)."
#~ msgstr ""

#~ msgid ""
#~ "Constructs an appropriately specialized "
#~ "QuOperator. If there are no edges, "
#~ "creates a QuScalar. If the are "
#~ "only output (input) edges, creates a "
#~ "QuVector (QuAdjointVector). Otherwise creates "
#~ "a QuOperator. Args:"
#~ msgstr ""

#~ msgid ""
#~ "out_edges: output edges. in_edges: in "
#~ "edges. ref_nodes: reference nodes for "
#~ "the tensor network (needed if there "
#~ "is a"
#~ msgstr ""

#~ msgid "scalar component)."
#~ msgstr ""

#~ msgid "ignore_edges: edges to ignore when checking the dimensionality of the"
#~ msgstr ""

#~ msgid "tensor network."
#~ msgstr ""

#~ msgid "The object."
#~ msgstr ""

#~ msgid "compute reduced density matrix from quantum state ``state``"
#~ msgstr ""

#~ msgid "Return expm of ``a``, matrix exponential."
#~ msgstr ""

#~ msgid "Return cos of ``a``."
#~ msgstr ""

#~ msgid "cos of ``a``"
#~ msgstr ""

#~ msgid "Return 1.j in as tensor comoatible with backend."
#~ msgstr ""

#~ msgid "Return elementwise imaginary value of ``a``."
#~ msgstr ""

#~ msgid "Return boolean on whether ``a`` is a tensor in backend package."
#~ msgstr ""

#~ msgid "Return kronecker product of two matrix ``a`` and ``b``."
#~ msgstr ""

#~ msgid "Return numpy array of tensor ``a``, may not work in jitted function."
#~ msgstr ""

#~ msgid "Return elementwise real value of ``a``."
#~ msgstr ""

#~ msgid "Return sin of ``a``."
#~ msgstr ""

#~ msgid "sin of ``a``"
#~ msgstr ""

#~ msgid "Solve linear system Ax=b and return x"
#~ msgstr ""

#~ msgid ""
#~ "This function is jittable in theory. "
#~ "But only jax+GPU combination is "
#~ "recommended for jit, since the graph "
#~ "building time is too long for "
#~ "other backend options, though the "
#~ "running time of the function is "
#~ "very fast for every case."
#~ msgstr ""

#~ msgid ""
#~ "random tensor between 0 or 1, "
#~ "defaults to None, the random number "
#~ "will generated automatically"
#~ msgstr ""

#~ msgid "[WIP], check whether the circuit is legal"
#~ msgstr ""

#~ msgid ""
#~ "middle measurement in z basis on "
#~ "the circuit, note the wavefunction "
#~ "output is not normalized with "
#~ "``mid_measurement`` involved, one should "
#~ "normalized the state manually if needed."
#~ msgstr ""

#~ msgid "Compute the output wavefunction from the circuit"
#~ msgstr ""

#~ msgid "To set the runtime numerical dtype of tensors"
#~ msgstr ""

#~ msgid ""
#~ "Apply a double qubit gate on "
#~ "adjacent qubits of Matrix Product States"
#~ " (MPS), truncation rule is specified "
#~ "by `set_truncation_rule`."
#~ msgstr ""

#~ msgid ""
#~ "Apply a double qubit gate on MPS,"
#~ " truncation rule is specified by "
#~ "`set_truncation_rule`."
#~ msgstr ""

#~ msgid ""
#~ "Apply a single qubit gate on MPS,"
#~ " the gate must be unitary, no "
#~ "truncation is needed."
#~ msgstr ""

#~ msgid "Compute expectation of the corresponding double qubit gate"
#~ msgstr ""

#~ msgid ""
#~ "Compute expectation of the direct "
#~ "product of the corresponding two gates"
#~ msgstr ""

#~ msgid ""
#~ "Middle measurement in z basis on "
#~ "the circuit, note the wavefunction "
#~ "output is not normalized with "
#~ "``mid_measurement`` involved, one should "
#~ "normalized the state manually if needed."
#~ msgstr ""

#~ msgid "compute entropy from given density matrix ``rho``"
#~ msgstr ""

#~ msgid ""
#~ "Simulate measuring each qubit of ``p``"
#~ " in the computational basis, producing "
#~ "output like that of ``qiskit``. "
#~ "Parameters ---------- state : vector or"
#~ " operator"
#~ msgstr ""

#~ msgid "C"
#~ msgstr ""

#~ msgid "int"
#~ msgstr ""

#~ msgid "phys_dim"
#~ msgstr ""

#~ msgid "int, optional"
#~ msgstr ""

#~ msgid "The assumed size of the subsystems of ``p``, defaults to 2 for qubits."
#~ msgstr ""

#~ msgid "results"
#~ msgstr ""

#~ msgid "Tuple[]"
#~ msgstr ""

#~ msgid ""
#~ "Compute the following with several input"
#~ " ``o`` as tensor or ``QuOperator``"
#~ msgstr ""

