# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The TensorCircuit Authors
# This file is distributed under the same license as the tensorcircuit
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version: tensorcircuit\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-04-13 15:28+0800\n"
"PO-Revision-Date: 2022-04-14 11:26+0800\n"
"Last-Translator: Xinghan Yang\n"
"Language: cn\n"
"Language-Team: cn <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"
"X-Generator: Poedit 1.6.11\n"

#: ../../source/faq.rst:2
msgid "Frequently Asked Questions"
msgstr "常见问题"

#: ../../source/faq.rst:5
msgid "How can I run TensorCircuit on GPU?"
msgstr "如何在 GPU 上运行 TensorCircuit"

#: ../../source/faq.rst:7
msgid ""
"This is done directly through the ML backend. GPU support is determined by "
"whether ML libraries are can run on GPU, we don't handle this within "
"tensorcircuit. It is the users' responsibility to configure a GPU-compatible "
"environment for these ML packages. Please refer to the installation "
"documentation for these ML packages and directly use the official "
"dockerfiles provided by TensorCircuit. With GPU compatible environment, we "
"can switch the use of GPU or CPU by a backend agnostic environment variable "
"``CUDA_VISIBLE_DEVICES``."
msgstr ""
"这是直接通过 ML 后端完成的。GPU 支持是直接取决于 ML 库是否可以在 GPU 上运行，"
"我们不在 tensorcircuit 中处理这个问题。为这些 ML 包配置与 GPU 兼容的环境是用"
"户的责任。请参考这些 ML 包的安装文档，或直接使用 TensorCircuit 提供的官方 "
"dockerfile。在 GPU 兼容的环境下，我们可以通过后端无关的环境变量 "
"``CUDA_VISIBLE_DEVICES`` 来切换使用 GPU 或 CPU。"

#: ../../source/faq.rst:12
msgid "Which ML framework backend should I use?"
msgstr "我应该使用哪个 ML 框架后端？"

#: ../../source/faq.rst:14
msgid ""
"Since the Numpy backend has no support for AD, if you want to evaluate the "
"circuit gradient, you must set the backend as one of the ML frameworks "
"beyond Numpy."
msgstr ""
"由于 Numpy 后端不支持自动微分，如果要评估电路梯度，必须将后端设置为 Numpy 之"
"外的 ML 框架之一。"

#: ../../source/faq.rst:16
msgid ""
"Since PyTorch has very limited support for vectorization and jit while our "
"package strongly depends on these features, it is not recommended to use. "
"Though one can always wrap a quantum function on another backend using a "
"PyTorch interface, say :py:meth:`tensorcircuit.interfaces.torch_interface`."
msgstr ""
"由于 PyTorch 对矢量化和 jit 的支持非常有限，而我们的包强烈依赖于这些特性，因"
"此不建议使用它。尽管总是可以使用 PyTorch 接口将量子函数包装在另一个后端，例"
"如：py:meth:` 张量电路.interfaces.torch_interface`。"

#: ../../source/faq.rst:18
msgid ""
"In terms of the choice between TensorFlow and Jax backend, the better one "
"may depend on the use cases and one may want to benchmark both to pick the "
"better one. There is no one-for-all recommendation and this is why we "
"maintain the backend agnostic form of our software."
msgstr ""
"就 TensorFlow 和 Jax 后端之间的选择而言，后端的好坏可能取决于用例，并且需要对"
"两者进行基准测试以可能选择更好的后端。没有一种万能的建议，这就是为什么我们维"
"持我们的软件支持不同的后端的形式。"

#: ../../source/faq.rst:20
msgid "Some general rules of thumb:"
msgstr "一些一般的经验法则："

#: ../../source/faq.rst:22
msgid ""
"On both CPU and GPU, the running time of a jitted function is faster for jax "
"backend."
msgstr "在 CPU 和 GPU 上，对于 jax 后端来说，jitted 函数的运行时间更快。"

#: ../../source/faq.rst:24
msgid "But on GPU, jit staging time is usually much longer for jax backend."
msgstr "但在 GPU 上，jax 后端的 jit 时间通常要长得多。"

#: ../../source/faq.rst:26
msgid ""
"For hybrid machine learning tasks, TensorFlow has a better ML ecosystem and "
"reusable classical ML models."
msgstr ""
"对于混合机器学习任务，TensorFlow 拥有更好的 ML 生态系统和可重用的经典 ML 模"
"型。"

#: ../../source/faq.rst:28
msgid ""
"Jax has some built-in advanced features that are lacking in TensorFlow, such "
"as checkpoint in AD and pmap for distributed computing."
msgstr ""
"Jax 具有一些 TensorFlow 所缺乏的内置高级功能，例如自动微分中的检查点和用于分"
"布式计算的 pmap。"

#: ../../source/faq.rst:30
msgid ""
"Jax is much insensitive to dtype where type promotion is handled "
"automatically which means easier debugging."
msgstr "Jax 对自动处理类型提升的 dtype 非常不敏感，这意味着更容易调试。"

#: ../../source/faq.rst:32
msgid ""
"TensorFlow can cache the jitted function on the disk via SavedModel, which "
"further amortizes the staging time."
msgstr ""
"TensorFlow 可以通过 SavedModel 将 jitted 函数缓存在磁盘上，从而进一步摊销编译"
"时间。"

#: ../../source/faq.rst:36
msgid ""
"What is the counterpart of ``QuantumLayer`` for PyTorch and Jax backend?"
msgstr "PyTorch 和 Jax 后端的 QuantumLayer 对应的是什么？"

#: ../../source/faq.rst:38
msgid ""
"Since PyTorch doesn't have mature vmap and jit support and Jax doesn't have "
"native classical ML layers, we highly recommend TensorFlow as the backend "
"for quantum-classical hybrid machine learning tasks, where ``QuantumLayer`` "
"plays an important role. For PyTorch, we can in principle wrap the "
"corresponding quantum function into a PyTorch module, but we currently have "
"no built-in support for this wrapper. In terms of the Jax backend, we highly "
"suggested keeping the functional programming paradigm for such machine "
"learning tasks. Besides, it is worth noting that, jit and vmap are "
"automatically taken care of in ``QuantumLayer``."
msgstr ""
"由于 PyTorch 没有成熟的 vmap 和 jit 支持，而且 Jax 没有原生的经典 ML 层，我们"
"强烈推荐 TensorFlow 作为量子经典混合机器学习任务的后端，其中 QuantumLayer 起"
"着重要作用。 对于 PyTorch，我们原则上可以将相应的量子函数包装到 PyTorch 模块"
"中，但我们目前没有内置支持这个包装器。在 Jax 后端方面，我们强烈建议保留函数式"
"编程范式来处理此类机器学习任务。此外，值得注意的是，jit 和 vmap 在 "
"QuantumLayer 中是自动处理的。"

#: ../../source/faq.rst:44
msgid "When do I need to customize the contractor and how?"
msgstr "我什么时候需要定制 contractor 以及如何定制？"

#: ../../source/faq.rst:46
msgid ""
"As a rule of thumb, for the circuit with qubit counts larger than 16 and "
"circuit depth larger than 8, customized contraction may outperform the "
"default built-in greedy contraction strategy."
msgstr ""
"根据经验，对于量子比特数大于 16 且电路深度大于 8 的电路，自定义收缩可能优于默"
"认的内置贪婪收缩策略。"

#: ../../source/faq.rst:48
msgid ""
"To set up or not set up the customized contractor is about a trade-off "
"between the time on contraction pathfinding and the time on the real "
"contraction via matmul."
msgstr ""
"设置或不设置定制 contractor 是在收缩寻路时间和通过 matmul 实际收缩时间之间进"
"行权衡。"

#: ../../source/faq.rst:50
msgid ""
"The customized contractor costs much more time than the default contractor "
"in terms of contraction path searching, and via the path it finds, the real "
"contraction can take less time and space."
msgstr ""
"在收缩路径搜索方面，定制 contractor 比默认 contractor 花费的时间要多得多。并"
"且通过它找到的路径比真正的收缩找到的路径花费更少的时间和空间。"

#: ../../source/faq.rst:52
msgid ""
"If the circuit simulation time is the bottleneck of the whole workflow, one "
"can always try customized contractors to see whether there is some "
"performance improvement."
msgstr ""
"如果电路仿真时间是整个工作流程的瓶颈，总是可以尝试定制 contractor，看看是否有"
"一些性能改进。"

#: ../../source/faq.rst:54
msgid ""
"We recommend to using `cotengra library <https://cotengra.readthedocs.io/en/"
"latest/index.html>`_ to set up the contractor, since there are lots of "
"interesting hyperparameters to tune, we can achieve a better trade-off "
"between the time on contraction path search and the time on the real tensor "
"network contraction."
msgstr ""
"我们建议使用 `cotengra library <https://cotengra.readthedocs.io/en/latest/"
"index.html>`_ 来设置 contractor，因为有很多有趣的超参数需要调整，我们可以实现"
"更好的收缩路径搜索时间和真实张量网络收缩时间之间的权衡。"

#: ../../source/faq.rst:56
msgid ""
"It is also worth noting that for jitted function which we usually use, the "
"contraction path search is only called at the first run of the function, "
"which further amortizes the time and favors the use of a highly customized "
"contractor."
msgstr ""
"\"“另外值得注意的是，对于我们通常使用的 jitted 函数，收缩路径搜索只在函数第一"
"次运行时调用，这进一步摊销了时间，有利于使用高度定制的 contractor。"

#: ../../source/faq.rst:58
msgid ""
"In terms of how-to on contractor setup, please refer to :ref:`quickstart:"
"Setup the Contractor`."
msgstr ""
"关于如何设置 contractor，请参阅 :ref:`quickstart:Setup the Contractor`。"

#: ../../source/faq.rst:61
msgid ""
"Is there some API less cumbersome than ``expectation`` for Pauli string?"
msgstr "对于 Pauli 字符串，有没有比 ``expectation`` 更简单的 API？"

#: ../../source/faq.rst:63
msgid ""
"Say we want to measure something like :math:`\\langle X_0Z_1Y_2Z_4 \\rangle` "
"for a six-qubit system, the general ``expectation`` API may seem to be "
"cumbersome. So one can try one of the following options:"
msgstr ""
"假设我们要为六量子位系统测量  :math:`\\langle X_0Z_1Y_2Z_4 \\rangle` 的东西，"
"一般的 ``expectation`` API 可能看起来很麻烦。所以可以尝试以下选项之一 :"

#: ../../source/faq.rst:66
msgid "``c.expectation_ps(x=[0], y=[2], z=[1, 4])``"
msgstr "``c.expectation_ps(x=[0], y=[2], z=[1, 4])``"

#: ../../source/faq.rst:68
msgid ""
"``tc.templates.measurements.parameterized_measurements(c, np.array([1, 3, 2, "
"0, 3, 0]), onehot=True)``"
msgstr ""
"``tc.templates.measurements.parameterized_measurements(c, np.array([1, 3, 2, "
"0, 3, 0]), onehot=True)``"

#: ../../source/faq.rst:71
msgid ""
"Can I apply quantum operation based on previous classical measurement "
"results in TensorCircuit?"
msgstr "我可以根据线路中的经典测量结果来决定后续的量子门操作吗？"

#: ../../source/faq.rst:73
msgid "Try the following: (the pipeline is even fully jittable!)"
msgstr "尝试以下方法：（方法甚至完全可以即时编译！）"

#: ../../source/faq.rst:82
msgid ""
"``cond_measurement`` will return 0 or 1 based on the measurement result on z-"
"basis, and ``conditional_gate`` applies gate_list[r] on the circuit."
msgstr ""
"``cond_measurement`` 将根据 z 基础上的测量结果返回 0 或 1，并且 "
"``conditional_gate`` 在电路上应用 gate_list[r]。"
