# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2020, The TensorCircuit Authors
# This file is distributed under the same license as the tensorcircuit
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2022.
#
msgid ""
msgstr ""
"Project-Id-Version:  tensorcircuit\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2022-04-13 15:28+0800\n"
"PO-Revision-Date: 2022-04-13 15:02+0800\n"
"Last-Translator: Xinghan Yang\n"
"Language: cn\n"
"Language-Team: cn <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.1\n"

#: ../../source/faq.rst:2
msgid "Frequently Asked Questions"
msgstr "常问问题"

#: ../../source/faq.rst:5
msgid "How can I run TensorCircuit on GPU?"
msgstr "如何在 GPU 上运行 TensorCircuit"

#: ../../source/faq.rst:7
msgid ""
"This is done directly through the ML backend. GPU support is determined "
"by whether ML libraries are can run on GPU, we don't handle this within "
"tensorcircuit. It is the users' responsibility to configure a GPU-"
"compatible environment for these ML packages. Please refer to the "
"installation documentation for these ML packages and directly use the "
"official dockerfiles provided by TensorCircuit. With GPU compatible "
"environment, we can switch the use of GPU or CPU by a backend agnostic "
"environment variable ``CUDA_VISIBLE_DEVICES``."
msgstr ""

#: ../../source/faq.rst:12
msgid "Which ML framework backend should I use?"
msgstr ""

#: ../../source/faq.rst:14
msgid ""
"Since the Numpy backend has no support for AD, if you want to evaluate "
"the circuit gradient, you must set the backend as one of the ML "
"frameworks beyond Numpy."
msgstr ""

#: ../../source/faq.rst:16
msgid ""
"Since PyTorch has very limited support for vectorization and jit while "
"our package strongly depends on these features, it is not recommended to "
"use. Though one can always wrap a quantum function on another backend "
"using a PyTorch interface, say "
":py:meth:`tensorcircuit.interfaces.torch_interface`."
msgstr ""

#: ../../source/faq.rst:18
msgid ""
"In terms of the choice between TensorFlow and Jax backend, the better one"
" may depend on the use cases and one may want to benchmark both to pick "
"the better one. There is no one-for-all recommendation and this is why we"
" maintain the backend agnostic form of our software."
msgstr ""

#: ../../source/faq.rst:20
msgid "Some general rules of thumb:"
msgstr ""

#: ../../source/faq.rst:22
msgid ""
"On both CPU and GPU, the running time of a jitted function is faster for "
"jax backend."
msgstr ""

#: ../../source/faq.rst:24
msgid "But on GPU, jit staging time is usually much longer for jax backend."
msgstr ""

#: ../../source/faq.rst:26
msgid ""
"For hybrid machine learning tasks, TensorFlow has a better ML ecosystem "
"and reusable classical ML models."
msgstr ""

#: ../../source/faq.rst:28
msgid ""
"Jax has some built-in advanced features that are lacking in TensorFlow, "
"such as checkpoint in AD and pmap for distributed computing."
msgstr ""

#: ../../source/faq.rst:30
msgid ""
"Jax is much insensitive to dtype where type promotion is handled "
"automatically which means easier debugging."
msgstr ""

#: ../../source/faq.rst:32
msgid ""
"TensorFlow can cache the jitted function on the disk via SavedModel, "
"which further amortizes the staging time."
msgstr ""

#: ../../source/faq.rst:36
msgid "What is the counterpart of ``QuantumLayer`` for PyTorch and Jax backend?"
msgstr ""

#: ../../source/faq.rst:38
msgid ""
"Since PyTorch doesn't have mature vmap and jit support and Jax doesn't "
"have native classical ML layers, we highly recommend TensorFlow as the "
"backend for quantum-classical hybrid machine learning tasks, where "
"``QuantumLayer`` plays an important role. For PyTorch, we can in "
"principle wrap the corresponding quantum function into a PyTorch module, "
"but we currently have no built-in support for this wrapper. In terms of "
"the Jax backend, we highly suggested keeping the functional programming "
"paradigm for such machine learning tasks. Besides, it is worth noting "
"that, jit and vmap are automatically taken care of in ``QuantumLayer``."
msgstr ""

#: ../../source/faq.rst:44
msgid "When do I need to customize the contractor and how?"
msgstr ""

#: ../../source/faq.rst:46
msgid ""
"As a rule of thumb, for the circuit with qubit counts larger than 16 and "
"circuit depth larger than 8, customized contraction may outperform the "
"default built-in greedy contraction strategy."
msgstr ""

#: ../../source/faq.rst:48
msgid ""
"To set up or not set up the customized contractor is about a trade-off "
"between the time on contraction pathfinding and the time on the real "
"contraction via matmul."
msgstr ""

#: ../../source/faq.rst:50
msgid ""
"The customized contractor costs much more time than the default "
"contractor in terms of contraction path searching, and via the path it "
"finds, the real contraction can take less time and space."
msgstr ""

#: ../../source/faq.rst:52
msgid ""
"If the circuit simulation time is the bottleneck of the whole workflow, "
"one can always try customized contractors to see whether there is some "
"performance improvement."
msgstr ""

#: ../../source/faq.rst:54
msgid ""
"We recommend to using `cotengra library "
"<https://cotengra.readthedocs.io/en/latest/index.html>`_ to set up the "
"contractor, since there are lots of interesting hyperparameters to tune, "
"we can achieve a better trade-off between the time on contraction path "
"search and the time on the real tensor network contraction."
msgstr ""

#: ../../source/faq.rst:56
msgid ""
"It is also worth noting that for jitted function which we usually use, "
"the contraction path search is only called at the first run of the "
"function, which further amortizes the time and favors the use of a highly"
" customized contractor."
msgstr ""

#: ../../source/faq.rst:58
msgid ""
"In terms of how-to on contractor setup, please refer to "
":ref:`quickstart:Setup the Contractor`."
msgstr ""

#: ../../source/faq.rst:61
msgid "Is there some API less cumbersome than ``expectation`` for Pauli string?"
msgstr ""

#: ../../source/faq.rst:63
msgid ""
"Say we want to measure something like :math:`\\langle X_0Z_1Y_2Z_4 "
"\\rangle` for a six-qubit system, the general ``expectation`` API may "
"seem to be cumbersome. So one can try one of the following options:"
msgstr ""

#: ../../source/faq.rst:66
msgid "``c.expectation_ps(x=[0], y=[2], z=[1, 4])``"
msgstr ""

#: ../../source/faq.rst:68
msgid ""
"``tc.templates.measurements.parameterized_measurements(c, np.array([1, 3,"
" 2, 0, 3, 0]), onehot=True)``"
msgstr ""

#: ../../source/faq.rst:71
msgid ""
"Can I apply quantum operation based on previous classical measurement "
"results in TensorCircuit?"
msgstr ""

#: ../../source/faq.rst:73
msgid "Try the following: (the pipeline is even fully jittable!)"
msgstr ""

#: ../../source/faq.rst:82
msgid ""
"``cond_measurement`` will return 0 or 1 based on the measurement result "
"on z-basis, and ``conditional_gate`` applies gate_list[r] on the circuit."
msgstr ""

#~ msgid ""
#~ "This is done directly through the "
#~ "ML backend. GPU support is totally "
#~ "determined by whether ML libraries are"
#~ " can run on GPU, we don't "
#~ "handle this within tensorcircuit. It is"
#~ " the users' responsibility to configure "
#~ "an GPU compatible environment for these"
#~ " ML packages. Please refer to the "
#~ "installation documentation for these ML "
#~ "packages and directly use official "
#~ "dockerfiles provided by TensorCircuit. With"
#~ " GPU compatible enviroment, we can "
#~ "switch the use of GPU or CPU "
#~ "by a backend agnostic environment "
#~ "variable ``CUDA_VISIBLE_DEVICES``."
#~ msgstr ""

#~ msgid ""
#~ "Since Numpy backend has no support "
#~ "for AD, if you want to evaluate"
#~ " the circuit gradient, you must set"
#~ " the backend as one of the ML"
#~ " framework beyond Numpy."
#~ msgstr ""

#~ msgid ""
#~ "Since PyTorch has very limited support"
#~ " for vectorization and jit while our"
#~ " package strongly depends on these "
#~ "features, it is not recommend to "
#~ "use. Though one can always wrap a"
#~ " quantum function on other backend "
#~ "using a PyTorch interface, say "
#~ ":py:meth:`tensorcircuit.interfaces.torch_interface`."
#~ msgstr ""

#~ msgid "Some general rule of thumb:"
#~ msgstr ""

#~ msgid ""
#~ "On both CPU and GPU, the running"
#~ " time of jitted function is faster"
#~ " for jax backend."
#~ msgstr ""

#~ msgid ""
#~ "For hybrid machine learning task, "
#~ "TensorFlow has a better ML ecosystem "
#~ "and reusable classical ML models."
#~ msgstr ""

#~ msgid ""
#~ "Jax has some built-in advanced "
#~ "features that is lack in TensorFlow, "
#~ "such as checkpoint in AD and pmap"
#~ " for distributed computing."
#~ msgstr ""

#~ msgid ""
#~ "TensorFlow can cached the jitted "
#~ "function on the disk via SavedModel, "
#~ "which further amortize the staging time."
#~ msgstr ""

#~ msgid ""
#~ "Since PyTorch doesn't have mature vmap"
#~ " and jit support and Jax doesn't "
#~ "have native classical ML layers, we "
#~ "highly recommend TensorFlow as the "
#~ "backend for quantum-classical hybrid "
#~ "machine learning tasks, where ``QuantumLayer``"
#~ " plays an important role. For "
#~ "PyTorch, we can in pricinple wrap "
#~ "the corresponding quantum function into "
#~ "a PyTorch module, but we currently "
#~ "has no built-in support for this"
#~ " wrapper. In terms of Jax backend,"
#~ " we highly suggested to keep the "
#~ "functional programming paradigm for such "
#~ "machine learning task. Besides, it is"
#~ " worthing noting that, jit and vmap"
#~ " is automatically taken care of in"
#~ " ``QuantumLayer``."
#~ msgstr ""

#~ msgid ""
#~ "As a rule of thumb, for circuit"
#~ " with qubit count larger than 16 "
#~ "and circuit depth larger than 8, "
#~ "customized contraction may outperform the "
#~ "deafult built-in greedy contraction "
#~ "strategy."
#~ msgstr ""

#~ msgid ""
#~ "To setup or not setup the "
#~ "customized contractor is about a "
#~ "trade-off between the time on "
#~ "contraction path finding and the time"
#~ " on the real contraction via matmul."
#~ msgstr ""

#~ msgid ""
#~ "The customized contractor cost much more"
#~ " time than default contractor in "
#~ "terms of contraction path searching, and"
#~ " via the path it finds, the "
#~ "real contraction can take less time "
#~ "and space."
#~ msgstr ""

#~ msgid ""
#~ "If the circuit simulation time is "
#~ "the bottleneck of the whole workflow,"
#~ " one can always try customized "
#~ "contractor to see whether there is "
#~ "some performance improvement."
#~ msgstr ""

#~ msgid ""
#~ "We recommend to use `cotengra library"
#~ " <https://cotengra.readthedocs.io/en/latest/index.html>`_ to"
#~ " setup the contractor, since there "
#~ "are lots of interesting hyperparameters "
#~ "to tune, we can achieve better "
#~ "trade-off between the time on "
#~ "contraction path search and the time "
#~ "on the real tensor network contraction."
#~ msgstr ""

#~ msgid ""
#~ "It is also worth noting that for"
#~ " jitted function which we usually "
#~ "use, the contraction path search is "
#~ "only called at the first run of"
#~ " the function, which further amortize "
#~ "the time and favor the use of "
#~ "highly customized contractor."
#~ msgstr ""

#~ msgid ""
#~ "Say we want to measure something "
#~ "like :math:`\\langle X_0Z_1Y_2Z_4 \\rangle` "
#~ "for a six-qubit system, the "
#~ "general ``expectation`` API may seems to"
#~ " be cumbersome. So one can try "
#~ "one of the following options:"
#~ msgstr ""

#~ msgid ""
#~ "Can I apply quantum operation based "
#~ "on previous classical measurement result "
#~ "in TensorCircuit?"
#~ msgstr ""

